@article{bassett21,
author = {Bassett, Debra},
year = {2021},
month = {02},
pages = {},
title = {Ctrl+Alt+Delete: The changing landscape of the uncanny valley and the fear of second loss},
volume = {40},
journal = {Current Psychology},
doi = {10.1007/s12144-018-0006-5}
}

@ARTICLE{9858635,
  author={Díaz-García, Lara and Reid, Andrew and Jackson-Camargo, Joseph C. and Windmill, James F. C.},
  journal={IEEE Sensors Journal}, 
  title={Toward a Bio-Inspired Acoustic Sensor: Achroia grisella’s Ear}, 
  year={2022},
  volume={22},
  number={18},
  pages={17746-17753},
  doi={10.1109/JSEN.2022.3197841}}

@misc{Hawesthoughts2023Colorblindness,
  author       = {Hawesthoughts},
  title        = {Color Blindness Wheels},
  howpublished = {Wikimedia Commons},
  month        = aug,
  year         = {2023},
  url          = {https://commons.wikimedia.org/wiki/File:Color_blindness_wheels.svg}
}

@INPROCEEDINGS{Berrezueta-Guzman2023PlagiarismDetection,
  author={Berrezueta-Guzman, Jonnathan and Paulsen, Markus and Krusche, Stephan},
  booktitle={2023 IEEE 35th International Conference on Software Engineering Education and Training (CSEE\&T)}, 
  title={Plagiarism Detection and its Effect on the Learning Outcomes}, 
  year={2023},
  volume={},
  number={},
  pages={99-108},
  keywords={Training;Plagiarism;Digital communication;Problem-solving;Engineering education;Programming profession;Information exchange;programming education;interactive learning;online training and education;software engineering education for novices;vision for education in the future},
  doi={10.1109/CSEET58097.2023.00021}}

@ARTICLE{Musick0HabitsLatex,
  author={Musick, Chad},
  journal={{ThinkSCIENCE}}, 
  title={5 Common Habits to Avoid in LaTeX}, 
  url={https://thinkscience.co.jp/en/articles/LaTeX-habits-to-avoid},
  note= {Accessed: 2025-07-09}}

@misc{Gelautz2023CVDrivingRobotics,
  author       = {Gelautz, M. and Schörkhuber, D. and Stoeva, D.},
  title        = {Computer Vision for Autonomous Driving and Robotics},
  howpublished = {Presentation, 30 Years ICG, Graz, Austria},
  year         = {2023},
  url          = {http://hdl.handle.net/20.500.12708/188254},
  note         = {Accessed: 2025-07-09}
}

# State of the Art Reviews

@ARTICLE{FromMethodstoApplicationsAReviewofDeep3DHumanMotionCapture,
  author={Niu, Zehai and Lu, Ke and Xue, Jian and Qin, Xiaoyu and Wang, Jinbao and Shao, Ling},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={From Methods to Applications: A Review of Deep 3D Human Motion Capture}, 
  year={2024},
  volume={34},
  number={11},
  pages={11340-11359},
  keywords={Motion capture;Three-dimensional displays;Pose estimation;Accuracy;Hardware;Cameras;Optical imaging;Deep learning;Pose estimation;Systematic literature review;Motion capture;deep learning;3D human pose estimation;literature review},
  doi={10.1109/TCSVT.2024.3423411}}

@Article{McInIndustryASystematicReview,
AUTHOR = {Menolotto, Matteo and Komaris, Dimitrios-Sokratis and Tedesco, Salvatore and O’Flynn, Brendan and Walsh, Michael},
TITLE = {Motion Capture Technology in Industrial Applications: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5687},
URL = {https://www.mdpi.com/1424-8220/20/19/5687},
PubMedID = {33028042},
ISSN = {1424-8220},
ABSTRACT = {The rapid technological advancements of Industry 4.0 have opened up new vectors for novel industrial processes that require advanced sensing solutions for their realization. Motion capture (MoCap) sensors, such as visual cameras and inertial measurement units (IMUs), are frequently adopted in industrial settings to support solutions in robotics, additive manufacturing, teleworking and human safety. This review synthesizes and evaluates studies investigating the use of MoCap technologies in industry-related research. A search was performed in the Embase, Scopus, Web of Science and Google Scholar. Only studies in English, from 2015 onwards, on primary and secondary industrial applications were considered. The quality of the articles was appraised with the AXIS tool. Studies were categorized based on type of used sensors, beneficiary industry sector, and type of application. Study characteristics, key methods and findings were also summarized. In total, 1682 records were identified, and 59 were included in this review. Twenty-one and 38 studies were assessed as being prone to medium and low risks of bias, respectively. Camera-based sensors and IMUs were used in 40% and 70% of the studies, respectively. Construction (30.5%), robotics (15.3%) and automotive (10.2%) were the most researched industry sectors, whilst health and safety (64.4%) and the improvement of industrial processes or products (17%) were the most targeted applications. Inertial sensors were the first choice for industrial MoCap applications. Camera-based MoCap systems performed better in robotic applications, but camera obstructions caused by workers and machinery was the most challenging issue. Advancements in machine learning algorithms have been shown to increase the capabilities of MoCap systems in applications such as activity and fatigue detection as well as tool condition monitoring and object recognition.},
DOI = {10.3390/s20195687}
}


# METHODS

@INPROCEEDINGS {PACE,
author = { Kocabas, Muhammed and Yuan, Ye and Molchanov, Pavlo and Guo, Yunrong and Black, Michael J. and Hilliges, Otmar and Kautz, Jan and Iqbal, Umar },
booktitle = { 2024 International Conference on 3D Vision (3DV) },
title = {{ PACE: Human and Camera Motion Estimation from in-the-wild Videos }},
year = {2024},
volume = {},
ISSN = {},
pages = {397-408},
abstract = { We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions. },
keywords = {Couplings;Simultaneous localization and mapping;Motion estimation;Dynamics;Cameras;Real-time systems;Trajectory},
doi = {10.1109/3DV62453.2024.00103},
url = {https://doi.ieeecomputersociety.org/10.1109/3DV62453.2024.00103},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =mar}

@inproceedings{FLAG,
  location = {New Orleans, {LA}, {USA}},
  title = {{FLAG}: Flow-based 3D Avatar Generation from Sparse Observations},
  rights = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  url = {https://ieeexplore.ieee.org/document/9879257/},
  doi = {10.1109/CVPR52688.2022.01290},
  shorttitle = {{FLAG}},
  abstract = {To represent people in mixed reality applications for collaboration and communication, we need to generate realistic and faithful avatar poses. However, the signal streams that can be applied for this task from head-mounted devices ({HMDs}) are typically limited to head pose and hand pose estimates. While these signals are valuable, they are an incomplete representation of the human body, making it challenging to generate a faithful full-body avatar. We address this challenge by developing a ﬂow-based generative model of the 3D human body from sparse observations, wherein we learn not only a conditional distribution of 3D human pose, but also a probabilistic mapping from observations to the latent space from which we can generate a plausible pose along with uncertainty estimates for the joints. We show that our approach is not only a strong predictive model, but can also act as an efﬁcient pose prior in different optimization settings where a good initial latent code plays a major role.},
  eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  pages = {13243--13252},
  booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher = {{IEEE}},
  author = {Aliakbarian, Sadegh and Cameron, Pashmina and Bogo, Federica and Fitzgibbon, Andrew and Cashman, Thomas J.},
  urldate = {2025-11-09},
  date = {2022-06},
  year = {2022},
  langid = {english},
}


@incollection{leonardis_avatarpose_2025,
	location = {Cham},
	title = {{AvatarPose}: Avatar-Guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos},
	volume = {15111},
	isbn = {978-3-031-73667-4 978-3-031-73668-1},
	url = {https://link.springer.com/10.1007/978-3-031-73668-1_13},
	shorttitle = {{AvatarPose}},
	abstract = {Despite progress in human motion capture, existing multiview methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of avatars to add penetration constraints. Moreover, both 3D poses and avatars are optimized in an alternating manner. Our experimental results demonstrate state-of-the-art performance on several public datasets.},
	pages = {215--233},
	booktitle = {Computer Vision – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Lu, Feichi and Dong, Zijian and Song, Jie and Hilliges, Otmar},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	urldate = {2025-11-09},
	date = {2024},
	langid = {english},
	doi = {10.1007/978-3-031-73668-1_13},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {PDF:/Users/emanum/Zotero/storage/9MISYHNV/Lu et al. - 2025 - AvatarPose Avatar-Guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Video.pdf:application/pdf},
}


@INPROCEEDINGS {simpoe,
author = { Yuan, Ye and Wei, Shih-En and Simon, Tomas and Kitani, Kris and Saragih, Jason },
booktitle = { 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ SimPoE: Simulated Character Control for 3D Human Pose Estimation }},
year = {2021},
volume = {},
ISSN = {},
pages = {7155-7165},
abstract = { Accurate estimation of 3D human motion from monocular video requires modeling both kinematics (body motion without physical forces) and dynamics (motion with physical forces). To demonstrate this, we present SimPoE, a Simulation-based approach for 3D human Pose Estimation, which integrates image-based kinematic inference and physics-based dynamics modeling. SimPoE learns a policy that takes as input the current-frame pose estimate and the next image frame to control a physically-simulated character to output the next-frame pose estimate. The policy contains a learnable kinematic pose refinement unit that uses 2D keypoints to iteratively refine its kinematic pose estimate of the next frame. Based on this refined kinematic pose, the policy learns to compute dynamics-based control (e.g., joint torques) of the character to advance the current-frame pose estimate to the pose estimate of the next frame. This design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning to achieve accurate and physically-plausible pose estimation. Furthermore, we propose a meta-control mechanism that dynamically adjusts the character’s dynamics parameters based on the character state to attain more accurate pose estimates. Experiments on large-scale motion datasets demonstrate that our approach establishes the new state of the art in pose accuracy while ensuring physical plausibility. },
keywords = {Solid modeling;Computer vision;Three-dimensional displays;Pose estimation;Dynamics;Kinematics;Reinforcement learning},
doi = {10.1109/CVPR46437.2021.00708},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00708},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@INPROCEEDINGS {VIBE,
author = { Kocabas, Muhammed and Athanasiou, Nikos and Black, Michael J. },
booktitle = { 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ VIBE: Video Inference for Human Body Pose and Shape Estimation }},
year = {2020},
volume = {},
ISSN = {},
pages = {5252-5262},
abstract = { Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose "Video Inference for Body Pose and Shape Estimation'' (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. We define a novel temporal network architecture with a self-attention mechanism and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE },
keywords = {Three-dimensional displays;Shape;Training;Two dimensional displays;Pose estimation;Predictive models},
doi = {10.1109/CVPR42600.2020.00530},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.00530},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}


# OPENPOSE START

@article{openPose,
  author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  year = {2019}
}

@inproceedings{openPose_2_simon2017hand,
  author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},
  year = {2017}
}

@inproceedings{openPose_3_cao2017realtime,
  author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  year = {2017}
}

@inproceedings{openPose_4_wei2016cpm,
  author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Convolutional pose machines},
  year = {2016}
}

@misc{openPose_github,
  author = {Zhe Cao and Gines Hidalgo Martinez and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  howpublished = {\url{https://github.com/CMU-Perceptual-Computing-Lab/openpose}},
  year = {2019}
}

@misc{openPose_docs,
  author = {Zhe Cao and Gines Hidalgo Martinez and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  title = {OpenPose: Documentation},
  howpublished = {\url{https://cmu-perceptual-computing-lab.github.io/openpose/web/html/doc/md_doc_02_output.html}},
  year = {2025}
}

# OPENPOSE END

@misc{mocopi,
author = {{Sony Interactive Entertainment Inc.}},
title = {mocopi},
howpublished = {\url{https://www.sony.co.jp/en/Products/mocopi-dev/en/}},
year = {2023},
note = {[accessed 25-November-2025]},
}


@article{chatzitofis2021democap,
  title={DeMoCap: Low-Cost Marker-Based Motion Capture},
  author={Chatzitofis, Anargyros and Zarpalas, Dimitrios and Daras, Petros and Kollias, Stefanos},
  journal={International Journal of Computer Vision},
  volume={129},
  number={12},
  pages={3338--3366},
  year={2021},
  publisher={Springer}
}

@article{mehta_xnect_2020,
author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Elgharib, Mohamed and Fua, Pascal and Seidel, Hans-Peter and Rhodin, Helge and Pons-Moll, Gerard and Theobalt, Christian},
title = {XNect: real-time multi-person 3D motion capture with a single RGB camera},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3386569.3392410},
doi = {10.1145/3386569.3392410},
abstract = {We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single RGB camera. It operates successfully in generic scenes which may contain occlusions by objects and by other people. Our method operates in subsequent stages. The first stage is a convolutional neural network (CNN) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals. We contribute a new architecture for this CNN, called SelecSLS Net, that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fullyconnected neural network turns the possibly partial (on account of occlusion) 2D pose and 3D pose features for each subject into a complete 3D pose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that do not produce joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.},
journal = {ACM Trans. Graph.},
month = aug,
articleno = {82},
numpages = {17},
keywords = {RGB, human body pose, monocular, motion capture, real-time}
}

@inproceedings{jiang2024egoposer,
  title={EgoPoser: Robust real-time egocentric pose estimation from sparse and intermittent observations everywhere},
  author={Jiang, Jiaxi and Streli, Paul and Meier, Manuel and Holz, Christian},
  booktitle={European Conference on Computer Vision},
  year={2024},
  organization={Springer}
}	



# Foundational References
@book{menache_understanding_2000,
	title = {Understanding motion capture for computer animation and video games},
	isbn = {978-0-12-490630-3},
	url = {http://archive.org/details/understandingmot0000mena_k0b1},
	abstract = {xiv, 238 p. : 24 cm; Includes index},
	pagetotal = {274},
	publisher = {San Diego, {CA} : Morgan Kaufmann},
	author = {Menache, Alberto},
	editora = {{Internet Archive}},
	editoratype = {collaborator},
	urldate = {2025-11-09},
  year = {2000},
	date = {2000},
	keywords = {Computer animation},
}


@article{wang_recent_2003,
	title = {Recent developments in human motion analysis},
	volume = {36},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320302001000},
	doi = {10.1016/S0031-3203(02)00100-0},
	abstract = {Visual analysis of human motion is currently one of the most active research topics in computer vision. This strong interest is driven by a wide spectrum of promising applications in many areas such as virtual reality, smart surveillance, perceptual interface, etc. Human motion analysis concerns the detection, tracking and recognition of people, and more generally, the understanding of human behaviors, from image sequences involving humans. This paper provides a comprehensive survey of research on computer vision based human motion analysis. The emphasis is on three major issues involved in a general human motion analysis system, namely human detection, tracking and activity understanding. Various methods for each issue are discussed in order to examine the state of the art. Finally, some research challenges and future directions are discussed.},
	pages = {585--601},
	number = {3},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Wang, Liang and Hu, Weiming and Tan, Tieniu},
	urldate = {2025-11-09},
	date = {2003-03},
	langid = {english},
}


@article{desmarais_review_2021,
	title = {A review of 3D human pose estimation algorithms for markerless motion capture},
	volume = {212},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314221001193},
	doi = {10.1016/j.cviu.2021.103275},
	abstract = {Human pose estimation is a very active research field, stimulated by its important applications in robotics, entertainment or health and sports sciences, among others. Advances in convolutional networks triggered noticeable improvements in 2D pose estimation, leading modern 3D markerless motion capture techniques to an average error per joint of 20 mm. However, with the proliferation of methods, it is becoming increasingly difficult to make an informed choice. Here, we review the leading human pose estimation methods of the past five years, focusing on metrics, benchmarks and method structures. We propose a taxonomy based on accuracy, speed and robustness that we use to classify de methods and derive directions for future research.},
	pages = {103275},
	journaltitle = {Computer Vision and Image Understanding},
	shortjournal = {Computer Vision and Image Understanding},
	author = {Desmarais, Yann and Mottet, Denis and Slangen, Pierre and Montesinos, Philippe},
	urldate = {2025-11-09},
	date = {2021-11-01},
	keywords = {3D human pose estimation, Convolutional neural networks, Survey},
}



# Online Vtuber Article 
@misc{hololive_setup,
	title = {At the Forefront! A Sneak Peek at COVER Corporation s New Studio!},
	url = {https://cover-corp.com/en/news/detail/20230511-03},
  howpublished = "\url{https://note.cover-corp.com/n/n50eb4a7180a6}",
  year = {2024},
  author = {{COVER Corporation}},
  note = "[accessed 09-November-2025]",
	urldate = {2025-11-09},
	date = {2023-05-11},
}


@misc{VICON,
author = {{Vicon Motion Systems Ltd.}},
title = {Vicon Motion Capture Systems},
howpublished = {\url{https://www.vicon.com/}},
year = {2025},
note = {[accessed 25-November-2025]},
}

@misc{optitrack,
  author={{NaturalPoint, Inc. DBA OptiTrack}},
  title={Optitrack},
  howpublished = {\url{https://optitrack.com}},
  note = {[accessed 25-November-2025]},
}

@misc{qualisys,
  author={{Qualisys AB}},
  title={Qualisys},
  howpublished = {\url{https://optitrack.com}},
  note = {[accessed 25-November-2025]},
}

# Datasets

@inproceedings{Harmony4D,
author = {Khirodkar, Rawal and Song, Jyun-Ting and Cao, Jinkun and Luo, Zhengyi and Kitani, Kris},
title = {Harmony4D: a video dataset for in-the-wild close human interactions},
year = {2024},
isbn = {9798331314385},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Understanding how humans interact with each other is key to building realistic multi-human virtual reality systems. This area remains relatively unexplored due to the lack of large-scale datasets. Recent datasets focusing on this issue mainly consist of activities captured entirely in controlled indoor environments with choreographed actions, significantly affecting their diversity. To address this, we introduce Harmony4D, a multi-view video dataset for human-human interaction featuring in-the-wild activities such as wrestling, dancing, MMA, and more. We use a flexible multi-view capture system to record these dynamic activities and provide annotations for human detection, tracking, 2D/3D pose estimation, and mesh recovery for closely interacting subjects. We propose a novel markerless algorithm to track 3D human poses in severe occlusion and close interaction to obtain our annotations with minimal manual intervention. Harmony4D consists of 1.66 million images and 3.32 million human instances from more than 20 synchronized cameras with 208 video sequences spanning diverse environments and 24 unique subjects. We rigorously evaluate existing state-of-the-art methods for mesh recovery and highlight their significant limitations in modeling close interaction scenarios. Additionally, we fine-tune a pre-trained HMR2.0 model on Harmony4D and demonstrate an improved performance of 54.8\% PVE in scenes with severe occlusion and contact. Code and data are available at https://jyuntins.github.io/harmony4d/.},
booktitle = {Proceedings of the 38th International Conference on Neural Information Processing Systems},
articleno = {3407},
numpages = {16},
location = {Vancouver, BC, Canada},
series = {NIPS '24}
}

@InProceedings{CHI3D,
author = {Fieraru, Mihai and Zanfir, Mihai and Oneata, Elisabeta and Popa, Alin-Ionut and Olaru, Vlad and Sminchisescu, Cristian},
title = {Three-Dimensional Reconstruction of Human Interactions},
booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{MOYO,
    title = {{3D} Human Pose Estimation via Intuitive Physics},
    author = {Tripathi, Shashank and M{\"u}ller, Lea and Huang, Chun-Hao P. and Taheri Omid and Black, Michael J. and Tzionas, Dimitrios},
    booktitle = {Conference on Computer Vision and Pattern Recognition ({CVPR})},
    pages = {4713--4725},
    year = {2023},
    url = {https://ipman.is.tue.mpg.de}
}

@inproceedings{Rich,
title = {Capturing and Inferring Dense Full-Body Human-Scene Contact},
author = {Huang, Chun-Hao P. and Yi, Hongwei and H{\"o}schle, Markus and Safroshkin, Matvey and Alexiadis, Tsvetelina and Polikovsky, Senya and Scharstein, Daniel and Black, Michael J.},
  booktitle = {Proceedings IEEE/CVF Conf.~on Computer Vision and Pattern Recognition (CVPR)},
  month = jun,
  year = {2022},
  pages = {13274-13285},
  month_numeric = {6}
}

@techreport{cg-2007-2,
   author = {M. M\"{u}ller and T. R\"{o}der and M. Clausen and B. Eberhardt and B. Kr\"{u}ger and A. Weber},
   title = {Documentation Mocap Database HDM05},
   number = {CG-2007-2},
   year = {2007},
   month = {June},
   institution = {Universit\"{a}t Bonn},
   ISSN = {1610-8892}
}

@InProceedings{coco,
author="Lin, Tsung-Yi
and Maire, Michael
and Belongie, Serge
and Hays, James
and Perona, Pietro
and Ramanan, Deva
and Doll{\'a}r, Piotr
and Zitnick, C. Lawrence",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Microsoft COCO: Common Objects in Context",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="740--755",
abstract="We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
isbn="978-3-319-10602-1"
}


@inproceedings{yin2023hi4d,
  author = {Yin, Yifei and Guo, Chen and Kaufmann, Manuel and Zarate, Juan and Song, Jie and Hilliges, Otmar}, 
  title = {Hi4D: 4D Instance Segmentation of Close Human Interaction}, 
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year = {2023}
}

@inproceedings{teufelgera2025HumanOLAT,
  title = {HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis},
  author = {Timo Teufel and Pulkit Gera and Xilong Zhou and Umar Iqbal and Pramod Rao and Jan Kautz and Vladislav Golyanik and Christian Theobalt},
  year = {2025},
  booktitle={International Conference on Computer Vision (ICCV)}
}

@article{10.1371/journal.pone.0253157,
    doi = {10.1371/journal.pone.0253157},
    author = {Ghorbani, Saeed AND Mahdaviani, Kimia AND Thaler, Anne AND Kording, Konrad AND Cook, Douglas James AND Blohm, Gunnar AND Troje, Nikolaus F.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {MoVi: A large multi-purpose human motion and video dataset},
    year = {2021},
    month = {06},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pone.0253157},
    pages = {1-15},
    abstract = {Large high-quality datasets of human body shape and kinematics lay the foundation for modelling and simulation approaches in computer vision, computer graphics, and biomechanics. Creating datasets that combine naturalistic recordings with high-accuracy data about ground truth body shape and pose is challenging because different motion recording systems are either optimized for one or the other. We address this issue in our dataset by using different hardware systems to record partially overlapping information and synchronized data that lend themselves to transfer learning. This multimodal dataset contains 9 hours of optical motion capture data, 17 hours of video data from 4 different points of view recorded by stationary and hand-held cameras, and 6.6 hours of inertial measurement units data recorded from 60 female and 30 male actors performing a collection of 21 everyday actions and sports movements. The processed motion capture data is also available as realistic 3D human meshes. We anticipate use of this dataset for research on human pose estimation, action recognition, motion modelling, gait analysis, and body shape reconstruction.},
    number = {6},
}

@article{sigal2009humaneva-8bd, 
  year     = {2009}, 
  title    = {{HumanEva}: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion}, 
  author   = {Sigal, Leonid and Balan, Alexandru O. and Black, Michael J.}, 
  journal  = {International Journal of Computer Vision}, 
  issn     = {0920-5691}, 
  doi      = {10.1007/s11263-009-0273-6}, 
  abstract = {While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of competing methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting {HumanEva} datasets contain multiple subjects performing a set of predefined actions with a number of repetitions. On the order of 40,000 frames of synchronized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60 Hz with an additional 37,000 time instants of pure motion capture data. A standard set of error measures is defined for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively standard Bayesian framework with optimization in the form of Sequential Importance Resampling and Annealed Particle Filtering. In the context of this baseline algorithm we explore a variety of likelihood functions, prior models of human motion and the effects of algorithm parameters. Our experiments suggest that image observation models and motion priors play important roles in performance, and that in a multi-view laboratory environment, where initialization is available, Bayesian filtering tends to perform well. The datasets and the software are made available to the research community. This infrastructure will support the development of new articulated motion and pose estimation algorithms, will provide a baseline for the evaluation and comparison of new methods, and will help establish the current state of the art in human pose estimation and tracking.}, 
  pages    = {4}, 
  number   = {1-2}, 
  volume   = {87}
}

@conference{AMASS:ICCV:2019,
  title = {{AMASS}: Archive of Motion Capture as Surface Shapes},
  author = {Mahmood, Naureen and Ghorbani, Nima and Troje, Nikolaus F. and Pons-Moll, Gerard and Black, Michael J.},
  booktitle = {International Conference on Computer Vision},
  pages = {5442--5451},
  month = oct,
  year = {2019},
  month_numeric = {10}
}

@misc{AMASS_ACCAD,
  title           = {{ACCAD MoCap Dataset}},
  author          = {{Advanced Computing Center for the Arts and Design}},
  url             = {https://accad.osu.edu/research/motion-lab/mocap-system-and-data}
}

@inproceedings{AMASS_BMLhandball,
  author          = {Helm, Fabian and Troje, Nikolaus and Reiser, Mathias and Munzert, Jörn},
  year            = {2015},
  month           = {01},
  pages           = {},
  title           = {Bewegungsanalyse getäuschter und nicht-getäuschter 7m-Würfe im Handball},
  journal         = {47. Jahrestagung der Arbeitsgemeinschaft für Sportpsychologie, Freiburg.}
}

@article{AMASS_BMLmovi,
  title           = {{MoVi}: A Large Multipurpose Motion and Video Dataset},
  author          = {Saeed Ghorbani and Kimia Mahdaviani and Anne Thaler and Konrad Kording and Douglas James Cook and Gunnar Blohm and Nikolaus F. Troje},
  year            = {2020},
  journal         = {arXiv preprint arXiv: 2003.01888}
}

@article{AMASS_BMLrub,
  title           = {Decomposing Biological Motion: {A} Framework for Analysis and Synthesis of Human Gait Patterns},
  author          = {Troje, Nikolaus F.},
  year            = 2002,
  month           = sep,
  journal         = {Journal of Vision},
  volume          = 2,
  number          = 5,
  pages           = {2--2},
  doi             = {10.1167/2.5.2},
  month_numeric   = 9
}

@misc{AMASS_CMU,
  title           = {{CMU MoCap Dataset}},
  author          = {{Carnegie Mellon University}},
  url             = {http://mocap.cs.cmu.edu}
}

@article{AMASS_DanceDB,
  author          = {Aristidou, Andreas and Shamir, Ariel and Chrysanthou, Yiorgos},
  title           = {Digital Dance Ethnography: {O}rganizing Large Dance Collections},
  journal         = {J. Comput. Cult. Herit.},
  issue_date      = {January 2020},
  volume          = {12},
  number          = {4},
  month           = nov,
  year            = {2019},
  issn            = {1556-4673},
  articleno       = {29},
  numpages        = {27},
  url             = {https://doi.org/10.1145/3344383},
  doi             = {10.1145/3344383},
  acmid           = {},
  publisher       = {Association for Computing Machinery},
  address         = {New York, NY, USA},
}

@inproceedings{AMASS_DFaust,
  title           = {Dynamic {FAUST}: {R}egistering Human Bodies in Motion},
  author          = {Bogo, Federica and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.},
  booktitle       = {IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
  month           = jul,
  year            = {2017},
  month_numeric   = {7}
}

@misc{AMASS_EyesJapanDataset,
  title           = {{Eyes Japan MoCap Dataset}},
  author          = {Eyes JAPAN Co. Ltd.},
  url             = {http://mocapdata.com}
}

@inproceedings{AMASS_GRAB,
  title           = {{GRAB}: A Dataset of Whole-Body Human Grasping of Objects},
  author          = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios},
  booktitle       = {European Conference on Computer Vision (ECCV)},
  year            = {2020},
  url             = {https://grab.is.tue.mpg.de}
}

@inproceedings{AMASS_GRAB-2,
  title           = {{ContactDB}: Analyzing and Predicting Grasp Contact via Thermal Imaging},
  author          = {Brahmbhatt, Samarth and Ham, Cusuh and Kemp, Charles C. and Hays, James},
  booktitle       = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year            = {2019},
  url             = {https://contactdb.cc.gatech.edu}
}

@techreport{AMASS_HDM05,
  author          = {M. M\"{u}ller and T. R\"{o}der and M. Clausen and B. Eberhardt and B. Kr\"{u}ger and A. Weber},
  title           = {Documentation Mocap Database HDM05},
  number          = {CG-2007-2},
  year            = {2007},
  month           = {June},
  institution     = {Universit\"{a}t Bonn},
  issn            = {1610-8892}
}

@article{AMASS_HUMAN4D,
  title           = {HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media},
  author          = {Chatzitofis, Anargyros and Saroglou, Leonidas and Boutis, Prodromos and Drakoulis, Petros and Zioulis, Nikolaos and Subramanyam, Shishir and Kevelham, Bart and Charbonnier, Caecilia and Cesar, Pablo and Zarpalas, Dimitrios and others},
  journal         = {IEEE Access},
  volume          = {8},
  pages           = {176241--176262},
  year            = {2020},
  publisher       = {IEEE}
}

@article{AMASS_HumanEva,
  title           = {{HumanEva}: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion},
  author          = {Sigal, L. and Balan, A. and Black, M. J.},
  journal         = {International Journal of Computer Vision},
  volume          = {87},
  number          = {1},
  pages           = {4--27},
  publisher       = {Springer Netherlands},
  month           = mar,
  year            = {2010},
  doi             = {},
  month_numeric   = {3}
}

@inproceedings{AMASS_KIT-CNRS-EKUT-WEIZMANN,
  author          = {Christian Mandery and \"Omer Terlemez and Martin Do and Nikolaus Vahrenkamp and Tamim Asfour},
  title           = {The {KIT} Whole-Body Human Motion Database},
  booktitle       = {International Conference on Advanced Robotics (ICAR)},
  pages           = {329--336},
  year            = {2015},
}

@article{AMASS_KIT-CNRS-EKUT-WEIZMANN-2,
  author          = {Christian Mandery and \"Omer Terlemez and Martin Do and Nikolaus Vahrenkamp and Tamim Asfour},
  title           = {Unifying Representations and Large-Scale Whole-Body Motion Databases for Studying Human Motion},
  pages           = {796--809},
  volume          = {32},
  number          = {4},
  journal         = {IEEE Transactions on Robotics},
  year            = {2016},
}

@inproceedings{AMASS_KIT-CNRS-EKUT-WEIZMANN-3,
  author          = {Franziska Krebs and Andre Meixner and Isabel Patzer and Tamim Asfour},
  title           = {The {KIT} Bimanual Manipulation Dataset},
  booktitle       = {IEEE/RAS International Conference on Humanoid Robots (Humanoids)},
  pages           = {499--506},
  year            = {2021},
}

@inproceedings{AMASS_MOYO,
  title           = {{3D} Human Pose Estimation via Intuitive Physics},
  author          = {Tripathi, Shashank and M{\"u}ller, Lea and Huang, Chun-Hao P. and Taheri Omid and Black, Michael J. and Tzionas, Dimitrios},
  booktitle       = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month           = {June},
  year            = {2023}
}

@article{AMASS_MoSh,
  title           = {{MoSh}: Motion and Shape Capture from Sparse Markers},
  author          = {Loper, Matthew M. and Mahmood, Naureen and Black, Michael J.},
  address         = {New York, NY, USA},
  publisher       = {ACM},
  month           = nov,
  number          = {6},
  volume          = {33},
  pages           = {220:1--220:13},
  abstract        = {Marker-based motion capture (mocap) is widely criticized as producing lifeless animations. We argue that important information about body surface motion is present in standard marker sets but is lost in extracting a skeleton. We demonstrate a new approach called MoSh (Motion and Shape capture), that automatically extracts this detail from mocap data. MoSh estimates body shape and pose together using sparse marker data by exploiting a parametric model of the human body. In contrast to previous work, MoSh solves for the marker locations relative to the body and estimates accurate body shape directly from the markers without the use of 3D scans; this effectively turns a mocap system into an approximate body scanner. MoSh is able to capture soft tissue motions directly from markers, by allowing body shape to vary over time. We evaluate the effect of different marker sets on pose and shape accuracy and propose a new sparse marker set for capturing soft-tissue motion. We illustrate MoSh by recovering body shape, pose, and soft-tissue motion from archival mocap data and using this to produce animations with subtlety and realism. We also show soft-tissue motion retargeting to new characters and show how to magnify the 3D deformations of soft tissue to create animations with appealing exaggerations.},
  journal         = {ACM Transactions on Graphics, (Proc. SIGGRAPH Asia)},
  url             = {http://doi.acm.org/10.1145/2661229.2661273},
  year            = {2014},
  doi             = {10.1145/2661229.2661273}
}

@inproceedings{AMASS_PosePrior,
  title           = {Pose-Conditioned Joint Angle Limits for {3D} Human Pose Reconstruction},
  author          = {Akhter, Ijaz and Black, Michael J.},
  booktitle       = { IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) 2015},
  month           = jun,
  abstract        = {The estimation of 3D human pose from 2D joint locations is central to many vision problems involving the analysis, of people in images and video. To address the fact that the problem is inherently ill posed, many methods impose a prior over human poses. Unfortunately these priors admit invalid poses because they do not model how joint-limits vary with pose. Here we make two key contributions. First, we collected a motion capture dataset that explores a wide range of human poses. From this we learn a pose-dependent model of joint limits that forms our prior. The dataset and the prior will be made publicly available. Second, we define a general parameterization of body pose and a new, multistage, method to estimate 3D pose from 2D joint locations that uses an over-complete dictionary of human poses. Our method shows good generalization while avoiding impossible poses. We quantitatively compare our method with recent work and show state-of-the-art results on 2D to 3D pose estimation using the CMU mocap dataset. We also show superior results on manual annotations on real images and automatic part-based detections on the Leeds sports pose dataset.},
  year            = {2015}
}

@misc{AMASS_SFU,
  title           = {{SFU Motion Capture Database}},
  author          = {Simon Fraser University and National University of Singapore},
  url             = {http://mocap.cs.sfu.ca/}
}

@inproceedings{AMASS_SOMA,
  title           = {{SOMA}: Solving Optical Marker-Based MoCap Automatically},
  author          = {Ghorbani, Nima and Black, Michael J.},
  booktitle       = {Proc. International Conference on Computer Vision (ICCV)},
  pages           = {11117--11126},
  month           = oct,
  year            = {2021},
  doi             = {},
  month_numeric   = {10}
}

@inproceedings{AMASS_TCDHands,
  author          = {Ludovic Hoyet and Kenneth Ryall and Rachel McDonnell and Carol O'Sullivan},
  title           = {Sleight of Hand: Perception of Finger Motion from Reduced Marker Sets},
  booktitle       = {Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games},
  year            = {2012},
  pages           = {79--86},
  doi             = {10.1145/2159616.2159629}
}

@inproceedings{AMASS_TotalCapture,
  author          = {Trumble, Matt and Gilbert, Andrew and Malleson, Charles and  Hilton, Adrian and Collomosse, John},
  title           = {{Total Capture}: 3D Human Pose Estimation Fusing Video and Inertial Sensors},
  booktitle       = {2017 British Machine Vision Conference (BMVC)},
  year            = {2017}
}

@inproceedings{AMASS_WheelPoser,
  title={WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users},
  author={Li, Yunzhi and Mollyn, Vimal and Yuan, Kuang and Carrington, Patrick},
  booktitle={Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--17},
  year={2024}
}

@misc{sfudataset,
  title = {SFU Motion Capture Database},
  author = {Simon Fraser University, National University of Singapore},
  year = {2011},
  howpublished = "\url{http://mocap.cs.sfu.ca/} ",
}


@article{Joo_2017_TPAMI,
  title={Panoptic Studio: A Massively Multiview System for Social Interaction Capture},
  author={Joo, Hanbyul and Simon, Tomas and Li, Xulong and Liu, Hao and Tan, Lei and Gui, Lin and Banerjee, Sean and Godisart, Timothy Scott and Nabbe, Bart and Matthews, Iain and Kanade, Takeo and Nobuhara, Shohei and Sheikh, Yaser},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017}
}

@inproceedings{singleshotmultiperson2018,
title = {Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB},
author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Sridhar, Srinath and Pons-Moll, Gerard and Theobalt, Christian},
booktitle = {3D Vision (3DV), 2018 Sixth International Conference on},
month = {sep},
year = {2018},
organization={IEEE},
url = {http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson}
}  

@ARTICLE{shi2024tro,
  author={Shi, Chenghao and Chen, Xieyuanli and Xiao, Junhao and Dai, Bin and Lu, Huimin},
  journal={IEEE Transactions on Robotics}, 
  title={Fast and Accurate Deep Loop Closing and Relocalization for Reliable LiDAR SLAM}, 
  year={2024},
  volume={40},
  number={},
  pages={2620-2640},
  doi={10.1109/TRO.2024.3386363}}

@INPROCEEDINGS {mp3d,
author = { Dabral, Rishabh and Gundavarapu, Nitesh B and Mitra, Rahul and Sharma, Abhishek and Ramakrishnan, Ganesh and Jain, Arjun },
booktitle = { 2019 International Conference on 3D Vision (3DV) },
title = {{ Multi-Person 3D Human Pose Estimation from Monocular Images }},
year = {2019},
volume = {},
ISSN = {},
pages = {405-414},
abstract = { Multi-person 3D human pose estimation from a single image is a challenging problem, especially for in-the-wild settings due to the lack of 3D annotated data. We propose HG-RCNN, a Mask-RCNN based network that also leverages the benefits of the Hourglass architecture for multi-person 3D Human Pose Estimation. A two-staged approach is presented that first estimates the 2D keypoints in every Region of Interest (RoI) and then lifts the estimated keypoints to 3D. Finally, the estimated 3D poses are placed in camera-coordinates using weak-perspective projection assumption and joint optimization of focal length and root translations. The result is a simple and modular network for multi-person 3D human pose estimation that does not require any multi-person 3D pose dataset. Despite its simple formulation, HG-RCNN achieves the state-of-the-art results on MuPoTS-3D while also approximating the 3D pose in the camera-coordinate system. },
keywords = {Three-dimensional displays;Pose estimation;Two dimensional displays;Heating systems;Pipelines;Cameras;Cognition},
doi = {10.1109/3DV.2019.00052},
url = {https://doi.ieeecomputersociety.org/10.1109/3DV.2019.00052},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =sep}

@ARTICLE{lcrnetplusplus,
  author={Rogez, Grégory and Weinzaepfel, Philippe and Schmid, Cordelia},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={LCR-Net++: Multi-Person 2D and 3D Pose Detection in Natural Images}, 
  year={2020},
  volume={42},
  number={5},
  pages={1146-1161},
  keywords={Three-dimensional displays;Two dimensional displays;Pose estimation;Proposals;Joints;Heating systems;Training data;Human 3D pose estimation;2D pose estimation;detection;localization;classification;regression;CNN},
  doi={10.1109/TPAMI.2019.2892985}}



@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}}


@inproceedings{UltraInertialPoser,
  author = {Armani, Rayan and Qian, Changlin and Jiang, Jiaxi and Holz, Christian},
  title = {{Ultra Inertial Poser}: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging},
  year = {2024},
  isbn = {9798400705250},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3641519.3657465},
  doi = {10.1145/3641519.3657465},
  booktitle = {ACM SIGGRAPH 2024 Conference Papers},
  articleno = {51},
  numpages = {11},
  keywords = {Human pose estimation, IMU, UWB, sparse tracking},
  location = {Denver, CO, USA},
  series = {SIGGRAPH '24}
}


@misc{UltraInertialPoser_YT,
  author = {Armani, Rayan and Qian, Changlin and Jiang, Jiaxi and Holz, Christian},
  title = {{Ultra Inertial Poser}: Video Presentation},
  year = {2024},
  author = {{Ultra Inertial Poser}: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging},
  howpublished = {\url{https://www.youtube.com/watch?v=CMT2ixXq0HA}},
  note = {[accessed 25-November-2025]},
}


@article{TransPose,
author = {Yi, Xinyu and Zhou, Yuxiao and Xu, Feng},
title = {TransPose: real-time 3D human translation and pose estimation with six inertial sensors},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459786},
doi = {10.1145/3450626.3459786},
abstract = {Motion capture is facing some new possibilities brought by the inertial sensing technologies which do not suffer from occlusion or wide-range recordings as vision-based solutions do. However, as the recorded signals are sparse and quite noisy, online performance and global translation estimation turn out to be two key difficulties. In this paper, we present TransPose, a DNN-based approach to perform full motion capture (with both global translations and body poses) from only 6 Inertial Measurement Units (IMUs) at over 90 fps. For body pose estimation, we propose a multi-stage network that estimates leaf-to-full joint positions as intermediate results. This design makes the pose estimation much easier, and thus achieves both better accuracy and lower computation cost. For global translation estimation, we propose a supporting-foot-based method and an RNN-based method to robustly solve for the global translations with a confidence-based fusion technique. Quantitative and qualitative comparisons show that our method outperforms the state-of-the-art learning- and optimization-based methods with a large margin in both accuracy and efficiency. As a purely inertial sensor-based approach, our method is not limited by environmental settings (e.g., fixed cameras), making the capture free from common difficulties such as wide-range motion space and strong occlusion.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {86},
numpages = {13},
keywords = {real-time, pose estimation, inverse kinematics, RNN, IMU}
}

@article{EgoLocate2023,
    author = {Yi, Xinyu and Zhou, Yuxiao and Habermann, Marc and Golyanik, Vladislav and Pan, Shaohua and Theobalt, Christian and Xu, Feng},
    title = {EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors},
    journal={ACM Transactions on Graphics (TOG)},
    year = {2023},
    volume = {42},
    number = {4},
    numpages = {17},
    articleno = {76},
    publisher = {ACM}
}

@InProceedings{PIPCVPR2022,
  author = {Yi, Xinyu and Zhou, Yuxiao and Habermann, Marc and Shimada, Soshi and Golyanik, Vladislav and Theobalt, Christian and Xu, Feng},
  title = {Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2022}
}

# UWB

@Article{s19204466,
AUTHOR = {Ledergerber, Anton and D’Andrea, Raffaello},
TITLE = {Ultra-Wideband Angle of Arrival Estimation Based on Angle-Dependent Antenna Transfer Function},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4466},
URL = {https://www.mdpi.com/1424-8220/19/20/4466},
PubMedID = {31618897},
ISSN = {1424-8220},
ABSTRACT = {Ultra-wideband radio signals are used in communication, indoor localization and radar systems, due to the high data rates, the high resilience to fading and the fine temporal resolution that can be achieved with a large bandwidth. This paper introduces a new method to estimate the angle of arrival of ultra-wideband radio signals with which existing time-of-flight based localization and radar systems can be augmented at no additional hardware cost. The method does not require multiple transmitter or receiver antennas, or relative motion between transmitter and receiver. Instead, it is solely based on the angle-dependent impulse response function of ultra-wideband antennas. Datasets on which the method is evaluated are publicly available. The method is further applied to a localization problem and it is shown how a robot can self-localize solely based on these angle of arrival estimates, and how they can be combined with time-of-flight measurements. Even though existing angle of arrival techniques that use multiple antennas show better accuracy, the method presented herein looks promising enough to be developed further and could potentially lead to electronically and mechanically simpler angle of arrival estimation technology.},
DOI = {10.3390/s19204466}
}


@article{cuevas2025mamma,
  title={MAMMA: Markerless \& Automatic Multi-Person Motion Action Capture},
  author={Cuevas-Velasquez, Hanz and Yiannakidis, Anastasios and Shin, Soyong and Becherini, Giorgio and H{\"o}schle, Markus and Tesch, Joachim and Obersat, Taylor and Alexiadis, Tsvetelina and Black, Michael J},
  journal={arXiv preprint arXiv:2506.13040},
  year={2025}
}


@Misc{easymocap,  
    title = {EasyMoCap - Make human motion capture easier.},
    howpublished = {Github},  
    year = {2021},
    url = {https://github.com/zju3dv/EasyMocap}
}

@inproceedings{shuai2022multinb,
  title={Novel View Synthesis of Human Interactions from Sparse
Multi-view Videos},
  author={Shuai, Qing and Geng, Chen and Fang, Qi and Peng, Sida and Shen, Wenhao and Zhou, Xiaowei and Bao, Hujun},
  booktitle={SIGGRAPH Conference Proceedings},
  year={2022}
}

@inproceedings{lin2022efficient,
  title={Efficient Neural Radiance Fields for Interactive Free-viewpoint Video},
  author={Lin, Haotong and Peng, Sida and Xu, Zhen and Yan, Yunzhi and Shuai, Qing and Bao, Hujun and Zhou, Xiaowei},
  booktitle={SIGGRAPH Asia Conference Proceedings},
  year={2022}
}

@inproceedings{dong2021fast,
  title={Fast and Robust Multi-Person 3D Pose Estimation and Tracking from Multiple Views},
  author={Dong, Junting and Fang, Qi and Jiang, Wen and Yang, Yurou and Bao, Hujun and Zhou, Xiaowei},
  booktitle={T-PAMI},
  year={2021}
}
    
@inproceedings{dong2020motion,
  title={Motion capture from internet videos},
  author={Dong, Junting and Shuai, Qing and Zhang, Yuanqing and Liu, Xian and Zhou, Xiaowei and Bao, Hujun},
  booktitle={European Conference on Computer Vision},
  pages={210--227},
  year={2020},
  organization={Springer}
}

@inproceedings{peng2021neural,
  title={Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans},
  author={Peng, Sida and Zhang, Yuanqing and Xu, Yinghao and Wang, Qianqian and Shuai, Qing and Bao, Hujun and Zhou, Xiaowei},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{fang2021mirrored,
  title={Reconstructing 3D Human Pose by Watching Humans in the Mirror},
  author={Fang, Qi and Shuai, Qing and Dong, Junting and Bao, Hujun and Zhou, Xiaowei},
  booktitle={CVPR},
  year={2021}
}


# TOOLS
@article{loper_smpl_2015,
title = {{SMPL}: a skinned multi-person linear model},
volume = {34},
issn = {0730-0301},
url = {https://dl.acm.org/doi/10.1145/2816795.2818013},
doi = {10.1145/2816795.2818013},
shorttitle = {{SMPL}},
abstract = {We present a learned model of human body shape and pose-dependent shape variation that is more accurate than previous models and is compatible with existing graphics pipelines. Our Skinned Multi-Person Linear model ({SMPL}) is a skinned vertex-based model that accurately represents a wide variety of body shapes in natural human poses. The parameters of the model are learned from data including the rest pose template, blend weights, pose-dependent blend shapes, identity-dependent blend shapes, and a regressor from vertices to joint locations. Unlike previous models, the pose-dependent blend shapes are a linear function of the elements of the pose rotation matrices. This simple formulation enables training the entire model from a relatively large number of aligned 3D meshes of different people in different poses. We quantitatively evaluate variants of {SMPL} using linear or dual-quaternion blend skinning and show that both are more accurate than a Blend-{SCAPE} model trained on the same data. We also extend {SMPL} to realistically model dynamic soft-tissue deformations. Because it is based on blend skinning, {SMPL} is compatible with existing rendering engines and we make it available for research purposes.},
pages = {248:1--248:16},
number = {6},
journaltitle = {{ACM} Trans. Graph.},
author = {Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.},
urldate = {2025-10-26},
date = {2015-11-02},
year = {2015}
}

@inproceedings{SMPLX,
  title = {Expressive Body Capture: {3D} Hands, Face, and Body from a Single Image},
  author = {Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed A. A. and Tzionas, Dimitrios and Black, Michael J.},
  booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {10975--10985},
  year = {2019}
}


@misc{vmc_protocol_specification,
	title = {{VMC} Protocol specification},
  howpublished = {\url{https://protocol.vmc.info/english.html}},
  year = {2025},
  note = {[accessed 09-November-2025]},
  author = {{VMCProtocol}},
	abstract = {{VMCProtocol} - Easy-to-use motion capture protocol specifications for games, tools, distribution environments, etc.},
	urldate = {2025-10-26},
}

@misc{Mixamo, 
title   = {Mixamo}, 
author  = {{Adobe}},
url     = {https://www.mixamo.com/#/}, 
urldate = {2025-11-08}
}

@misc{VRM_consortium,
title   = {VRM Consortium},
url     = {https://vrm-consortium.org/en/},
howpublished = {\url{https://vrm-consortium.org/en/}},
year = {2025},
author = {{VRM Consortium}},
note = {[accessed 09-November-2025]},
urldate = {2025-11-09}
}

@misc{VRM_PDF,
title   = {VRM - Extending glTF for Portable Humanoid Avatars},
url     = {https://www.khronos.org/assets/uploads/developers/presentations/VRM-Extending-glTF-for-Portable-Humanoid-Avatars_SIGGRAPH-Asia_Nov19.pdf},
howpublished = {\url{https://www.khronos.org/assets/uploads/developers/presentations/VRM-Extending-glTF-for-Portable-Humanoid-Avatars_SIGGRAPH-Asia_Nov19.pdf}},
year = {2019},
author = {{VRM Consortium}},
note = {[accessed 23-December-2025]}
}

@misc{vrm_press_release,
title = {The Khronos Group and VRM Consortium Collaborate to Advance
International Standardization of the VRM 3D Avatar File Format},
howpublished = {\url{https://vrm-consortium.org/en/common/pdf/VRMC%20Khronos%20Press%20Release%2020241024.pdf}},
year = {2024},
author = {{VRM Consortium and Khronos Group}},
note = {[accessed 09-November-2025]},
date = {2024-10-24},
}

@misc{OpenSoundControl,
title   = {OpenSoundControl},
url     = {https://ccrma.stanford.edu/groups/osc/index.html},
howpublished = {\url{https://ccrma.stanford.edu/groups/osc/index.html}},
year = {2019},
author = {{VRM Consortium}},
note = {[accessed 23-December-2025]}
}


# Example for VMR and VMC

@misc{warudo_app,
  author = {Warudo Team},
  title = {Warudo - VRM/ VMC compatible motion capture application},
  howpublished = {\url{https://warudo.app/}},
  year = {2025},
  note = {[accessed 24-December-2025]},
}

@misc{vseeface_app,
  author = {VSeeFace Team},
  title = {VSeeFace - Free VRM/ VMC compatible motion capture application},
  howpublished = {\url{https://www.vseeface.icu/}},
  year = {2025},
  note = {[accessed 24-December-2025]},
}

https://hub.vroid.com/en

@misc{vroid_hub,
  author = {VRoid Team},
  title = {VRoid Hub - Free VRM compatible 3D avatar sharing platform},
  howpublished = {\url{https://hub.vroid.com/en}},
  year = {2025},
  note = {[accessed 24-December-2025]},
}

% vket https://vket.com/en/docs/submission_tips_vrm_file

@misc{vket_vrm,
  author = {Virtual Market Team},
  title = {Virtual Market - VRM compatible 3D avatar event platform},
  howpublished = {\url{https://vket.com/ene}},
  year = {2025},
  note = {[accessed 24-December-2025]},
}

% https://vgen.co/
@misc{vgen_vrm,
  author = {VGen Team},
  title = {VGen - a community and marketplace dedicated to empowering and protecting human creativity},
  howpublished = {\url{https://vgen.co/}},
  year = {2025},
  note = {[accessed 24-December-2025]},
}



@misc{fbx_format,
  author = {Adobe},
  title = {FBX Adaptable file format for 3D animation software},
  howpublished = {\url{https://www.autodesk.com/products/fbx/overview}},
  year = {2025},
  note = {[accessed 07-December-2025]},
}

@misc{Collada_format,
  author = {{Khronos Group}},
  title = {COLLADA: Collaborative Design Activity},
  howpublished = {\url{https://www.khronos.org/collada/}},
  year = {2025},
  note = {[accessed 07-December-2025]},
}

https://www.khronos.org/Gltf

@misc{gltf_format,
  author = {{Khronos Group}},
  title = {{GLTF: The 3D Asset Delivery Format}},
  howpublished = {\url{https://www.khronos.org/gltf/}},
  year = {2025},
  note = {[accessed 07-December-2025]},
}
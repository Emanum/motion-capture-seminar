@article{bassett21,
author = {Bassett, Debra},
year = {2021},
month = {02},
pages = {},
title = {Ctrl+Alt+Delete: The changing landscape of the uncanny valley and the fear of second loss},
volume = {40},
journal = {Current Psychology},
doi = {10.1007/s12144-018-0006-5}
}

@ARTICLE{9858635,
  author={Díaz-García, Lara and Reid, Andrew and Jackson-Camargo, Joseph C. and Windmill, James F. C.},
  journal={IEEE Sensors Journal}, 
  title={Toward a Bio-Inspired Acoustic Sensor: Achroia grisella’s Ear}, 
  year={2022},
  volume={22},
  number={18},
  pages={17746-17753},
  doi={10.1109/JSEN.2022.3197841}}

@misc{Hawesthoughts2023Colorblindness,
  author       = {Hawesthoughts},
  title        = {Color Blindness Wheels},
  howpublished = {Wikimedia Commons},
  month        = aug,
  year         = {2023},
  url          = {https://commons.wikimedia.org/wiki/File:Color_blindness_wheels.svg}
}

@INPROCEEDINGS{Berrezueta-Guzman2023PlagiarismDetection,
  author={Berrezueta-Guzman, Jonnathan and Paulsen, Markus and Krusche, Stephan},
  booktitle={2023 IEEE 35th International Conference on Software Engineering Education and Training (CSEE\&T)}, 
  title={Plagiarism Detection and its Effect on the Learning Outcomes}, 
  year={2023},
  volume={},
  number={},
  pages={99-108},
  keywords={Training;Plagiarism;Digital communication;Problem-solving;Engineering education;Programming profession;Information exchange;programming education;interactive learning;online training and education;software engineering education for novices;vision for education in the future},
  doi={10.1109/CSEET58097.2023.00021}}

@ARTICLE{Musick0HabitsLatex,
  author={Musick, Chad},
  journal={{ThinkSCIENCE}}, 
  title={5 Common Habits to Avoid in LaTeX}, 
  url={https://thinkscience.co.jp/en/articles/LaTeX-habits-to-avoid},
  note= {Accessed: 2025-07-09}}

@misc{Gelautz2023CVDrivingRobotics,
  author       = {Gelautz, M. and Schörkhuber, D. and Stoeva, D.},
  title        = {Computer Vision for Autonomous Driving and Robotics},
  howpublished = {Presentation, 30 Years ICG, Graz, Austria},
  year         = {2023},
  url          = {http://hdl.handle.net/20.500.12708/188254},
  note         = {Accessed: 2025-07-09}
}

# State of the Art Reviews

@ARTICLE{FromMethodstoApplicationsAReviewofDeep3DHumanMotionCapture,
  author={Niu, Zehai and Lu, Ke and Xue, Jian and Qin, Xiaoyu and Wang, Jinbao and Shao, Ling},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={From Methods to Applications: A Review of Deep 3D Human Motion Capture}, 
  year={2024},
  volume={34},
  number={11},
  pages={11340-11359},
  keywords={Motion capture;Three-dimensional displays;Pose estimation;Accuracy;Hardware;Cameras;Optical imaging;Deep learning;Pose estimation;Systematic literature review;Motion capture;deep learning;3D human pose estimation;literature review},
  doi={10.1109/TCSVT.2024.3423411}}

@Article{McInIndustryASystematicReview,
AUTHOR = {Menolotto, Matteo and Komaris, Dimitrios-Sokratis and Tedesco, Salvatore and O’Flynn, Brendan and Walsh, Michael},
TITLE = {Motion Capture Technology in Industrial Applications: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5687},
URL = {https://www.mdpi.com/1424-8220/20/19/5687},
PubMedID = {33028042},
ISSN = {1424-8220},
ABSTRACT = {The rapid technological advancements of Industry 4.0 have opened up new vectors for novel industrial processes that require advanced sensing solutions for their realization. Motion capture (MoCap) sensors, such as visual cameras and inertial measurement units (IMUs), are frequently adopted in industrial settings to support solutions in robotics, additive manufacturing, teleworking and human safety. This review synthesizes and evaluates studies investigating the use of MoCap technologies in industry-related research. A search was performed in the Embase, Scopus, Web of Science and Google Scholar. Only studies in English, from 2015 onwards, on primary and secondary industrial applications were considered. The quality of the articles was appraised with the AXIS tool. Studies were categorized based on type of used sensors, beneficiary industry sector, and type of application. Study characteristics, key methods and findings were also summarized. In total, 1682 records were identified, and 59 were included in this review. Twenty-one and 38 studies were assessed as being prone to medium and low risks of bias, respectively. Camera-based sensors and IMUs were used in 40% and 70% of the studies, respectively. Construction (30.5%), robotics (15.3%) and automotive (10.2%) were the most researched industry sectors, whilst health and safety (64.4%) and the improvement of industrial processes or products (17%) were the most targeted applications. Inertial sensors were the first choice for industrial MoCap applications. Camera-based MoCap systems performed better in robotic applications, but camera obstructions caused by workers and machinery was the most challenging issue. Advancements in machine learning algorithms have been shown to increase the capabilities of MoCap systems in applications such as activity and fatigue detection as well as tool condition monitoring and object recognition.},
DOI = {10.3390/s20195687}
}


# METHODS

@INPROCEEDINGS {PACE,
author = { Kocabas, Muhammed and Yuan, Ye and Molchanov, Pavlo and Guo, Yunrong and Black, Michael J. and Hilliges, Otmar and Kautz, Jan and Iqbal, Umar },
booktitle = { 2024 International Conference on 3D Vision (3DV) },
title = {{ PACE: Human and Camera Motion Estimation from in-the-wild Videos }},
year = {2024},
volume = {},
ISSN = {},
pages = {397-408},
abstract = { We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions. },
keywords = {Couplings;Simultaneous localization and mapping;Motion estimation;Dynamics;Cameras;Real-time systems;Trajectory},
doi = {10.1109/3DV62453.2024.00103},
url = {https://doi.ieeecomputersociety.org/10.1109/3DV62453.2024.00103},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =mar}

@inproceedings{FLAG,
  location = {New Orleans, {LA}, {USA}},
  title = {{FLAG}: Flow-based 3D Avatar Generation from Sparse Observations},
  rights = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  url = {https://ieeexplore.ieee.org/document/9879257/},
  doi = {10.1109/CVPR52688.2022.01290},
  shorttitle = {{FLAG}},
  abstract = {To represent people in mixed reality applications for collaboration and communication, we need to generate realistic and faithful avatar poses. However, the signal streams that can be applied for this task from head-mounted devices ({HMDs}) are typically limited to head pose and hand pose estimates. While these signals are valuable, they are an incomplete representation of the human body, making it challenging to generate a faithful full-body avatar. We address this challenge by developing a ﬂow-based generative model of the 3D human body from sparse observations, wherein we learn not only a conditional distribution of 3D human pose, but also a probabilistic mapping from observations to the latent space from which we can generate a plausible pose along with uncertainty estimates for the joints. We show that our approach is not only a strong predictive model, but can also act as an efﬁcient pose prior in different optimization settings where a good initial latent code plays a major role.},
  eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  pages = {13243--13252},
  booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher = {{IEEE}},
  author = {Aliakbarian, Sadegh and Cameron, Pashmina and Bogo, Federica and Fitzgibbon, Andrew and Cashman, Thomas J.},
  urldate = {2025-11-09},
  date = {2022-06},
  year = {2022},
  langid = {english},
}


@incollection{leonardis_avatarpose_2025,
	location = {Cham},
	title = {{AvatarPose}: Avatar-Guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos},
	volume = {15111},
	isbn = {978-3-031-73667-4 978-3-031-73668-1},
	url = {https://link.springer.com/10.1007/978-3-031-73668-1_13},
	shorttitle = {{AvatarPose}},
	abstract = {Despite progress in human motion capture, existing multiview methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of avatars to add penetration constraints. Moreover, both 3D poses and avatars are optimized in an alternating manner. Our experimental results demonstrate state-of-the-art performance on several public datasets.},
	pages = {215--233},
	booktitle = {Computer Vision – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Lu, Feichi and Dong, Zijian and Song, Jie and Hilliges, Otmar},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	urldate = {2025-11-09},
	date = {2025},
	langid = {english},
	doi = {10.1007/978-3-031-73668-1_13},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {PDF:/Users/emanum/Zotero/storage/9MISYHNV/Lu et al. - 2025 - AvatarPose Avatar-Guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Video.pdf:application/pdf},
}


@INPROCEEDINGS {simpoe,
author = { Yuan, Ye and Wei, Shih-En and Simon, Tomas and Kitani, Kris and Saragih, Jason },
booktitle = { 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ SimPoE: Simulated Character Control for 3D Human Pose Estimation }},
year = {2021},
volume = {},
ISSN = {},
pages = {7155-7165},
abstract = { Accurate estimation of 3D human motion from monocular video requires modeling both kinematics (body motion without physical forces) and dynamics (motion with physical forces). To demonstrate this, we present SimPoE, a Simulation-based approach for 3D human Pose Estimation, which integrates image-based kinematic inference and physics-based dynamics modeling. SimPoE learns a policy that takes as input the current-frame pose estimate and the next image frame to control a physically-simulated character to output the next-frame pose estimate. The policy contains a learnable kinematic pose refinement unit that uses 2D keypoints to iteratively refine its kinematic pose estimate of the next frame. Based on this refined kinematic pose, the policy learns to compute dynamics-based control (e.g., joint torques) of the character to advance the current-frame pose estimate to the pose estimate of the next frame. This design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning to achieve accurate and physically-plausible pose estimation. Furthermore, we propose a meta-control mechanism that dynamically adjusts the character’s dynamics parameters based on the character state to attain more accurate pose estimates. Experiments on large-scale motion datasets demonstrate that our approach establishes the new state of the art in pose accuracy while ensuring physical plausibility. },
keywords = {Solid modeling;Computer vision;Three-dimensional displays;Pose estimation;Dynamics;Kinematics;Reinforcement learning},
doi = {10.1109/CVPR46437.2021.00708},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00708},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@INPROCEEDINGS {VIBE,
author = { Kocabas, Muhammed and Athanasiou, Nikos and Black, Michael J. },
booktitle = { 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ VIBE: Video Inference for Human Body Pose and Shape Estimation }},
year = {2020},
volume = {},
ISSN = {},
pages = {5252-5262},
abstract = { Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose "Video Inference for Body Pose and Shape Estimation'' (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. We define a novel temporal network architecture with a self-attention mechanism and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE },
keywords = {Three-dimensional displays;Shape;Training;Two dimensional displays;Pose estimation;Predictive models},
doi = {10.1109/CVPR42600.2020.00530},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.00530},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}


# Old but good
@ARTICLE{openPose,
author={Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
journal={ IEEE Transactions on Pattern Analysis \& Machine Intelligence },
title={{ OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields }},
year={2021},
volume={43},
number={01},
ISSN={1939-3539},
pages={172-186},
abstract={ Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints. },
keywords={Two dimensional displays;Pose estimation;Detectors;Runtime;Kernel;Training},
doi={10.1109/TPAMI.2019.2929257},
url = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2929257},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}


@article{chatzitofis_democap_2021,
title = {{DeMoCap}: Low-Cost Marker-Based Motion Capture},
volume = {129},
issn = {1573-1405},
url = {https://doi.org/10.1007/s11263-021-01526-z},
doi = {10.1007/s11263-021-01526-z},
abstract = {Optical marker-based motion capture ({MoCap}) remains the predominant way to acquire high-fidelity articulated body motions. We introduce {DeMoCap}, the first data-driven approach for end-to-end marker-based {MoCap}, using only a sparse setup of spatio-temporally aligned, consumer-grade infrared-depth cameras. Trading off some of their typical features, our approach is the sole robust option for far lower-cost marker-based {MoCap} than high-end solutions. We introduce an end-to-end differentiable markers-to-pose model to solve a set of challenges such as under-constrained position estimates, noisy input data and spatial configuration invariance. We simultaneously handle depth and marker detection noise, label and localize the markers, and estimate the 3D pose by introducing a novel spatial 3D coordinate regression technique under a multi-view rendering and supervision concept. {DeMoCap} is driven by a special dataset captured with 4 spatio-temporally aligned low-cost Intel {RealSense} D415 sensors and a 24 {MXT}40S camera professional {MoCap} system, used as input and ground truth, respectively.},
pages = {3338--3366},
number = {12},
journaltitle = {International Journal of Computer Vision},
shortjournal = {International Journal of Computer Vision},
author = {Chatzitofis, Anargyros and Zarpalas, Dimitrios and Daras, Petros and Kollias, Stefanos},
date = {2021-12-01},
}

@article{mehta_xnect_2020,
	title = {{XNect}: real-time multi-person 3D motion capture with a single {RGB} camera},
	volume = {39},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3386569.3392410},
	doi = {10.1145/3386569.3392410},
	shorttitle = {{XNect}},
	abstract = {We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single {RGB} camera. It operates successfully in generic scenes which may contain occlusions by objects and by other people. Our method operates in subsequent stages. The first stage is a convolutional neural network ({CNN}) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals. We contribute a new architecture for this {CNN}, called {SelecSLS} Net, that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fullyconnected neural network turns the possibly partial (on account of occlusion) 2D pose and 3D pose features for each subject into a complete 3D pose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that do not produce joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.},
	pages = {82:82:1--82:82:17},
	number = {4},
	journaltitle = {{ACM} Trans. Graph.},
	author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Elgharib, Mohamed and Fua, Pascal and Seidel, Hans-Peter and Rhodin, Helge and Pons-Moll, Gerard and Theobalt, Christian},
	urldate = {2025-10-19},
	date = {2020-08-12},
	file = {Full Text PDF:/Users/emanum/Zotero/storage/6D625ETP/Mehta et al. - 2020 - XNect real-time multi-person 3D motion capture with a single RGB camera.pdf:application/pdf},
}

@inproceedings{jiang2024egoposer,
  title={EgoPoser: Robust real-time egocentric pose estimation from sparse and intermittent observations everywhere},
  author={Jiang, Jiaxi and Streli, Paul and Meier, Manuel and Holz, Christian},
  booktitle={European Conference on Computer Vision},
  year={2024},
  organization={Springer}
}	


# TOOLS
@article{loper_smpl_2015,
title = {{SMPL}: a skinned multi-person linear model},
volume = {34},
issn = {0730-0301},
url = {https://dl.acm.org/doi/10.1145/2816795.2818013},
doi = {10.1145/2816795.2818013},
shorttitle = {{SMPL}},
abstract = {We present a learned model of human body shape and pose-dependent shape variation that is more accurate than previous models and is compatible with existing graphics pipelines. Our Skinned Multi-Person Linear model ({SMPL}) is a skinned vertex-based model that accurately represents a wide variety of body shapes in natural human poses. The parameters of the model are learned from data including the rest pose template, blend weights, pose-dependent blend shapes, identity-dependent blend shapes, and a regressor from vertices to joint locations. Unlike previous models, the pose-dependent blend shapes are a linear function of the elements of the pose rotation matrices. This simple formulation enables training the entire model from a relatively large number of aligned 3D meshes of different people in different poses. We quantitatively evaluate variants of {SMPL} using linear or dual-quaternion blend skinning and show that both are more accurate than a Blend-{SCAPE} model trained on the same data. We also extend {SMPL} to realistically model dynamic soft-tissue deformations. Because it is based on blend skinning, {SMPL} is compatible with existing rendering engines and we make it available for research purposes.},
pages = {248:1--248:16},
number = {6},
journaltitle = {{ACM} Trans. Graph.},
author = {Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.},
urldate = {2025-10-26},
date = {2015-11-02},
}


@misc{vmc_protocol_specification,
	title = {{VMC} Protocol specification},
  howpublished = {\url{https://protocol.vmc.info/english.html}},
  year = {2025},
  note = {[accessed 09-November-2025]},
  author = {{VMCProtocol}},
	abstract = {{VMCProtocol} - Easy-to-use motion capture protocol specifications for games, tools, distribution environments, etc.},
	urldate = {2025-10-26},
}

@misc{Mixamo, 
title   = {Mixamo}, 
author  = {{Adobe}},
url     = {https://www.mixamo.com/#/}, 
urldate = {2025-11-08}
}

@misc{VRM_consortium,
title   = {VRM Consortium},
url     = {https://vrm-consortium.org/en/},
howpublished = {\url{https://vrm-consortium.org/en/}},
year = {2025},
author = {{VRM Consortium}},
note = {[accessed 09-November-2025]},
urldate = {2025-11-09}
}

@misc{vrm_press_release,
title = {The Khronos Group and VRM Consortium Collaborate to Advance
International Standardization of the VRM 3D Avatar File Format},
howpublished = {\url{https://vrm-consortium.org/en/common/pdf/VRMC%20Khronos%20Press%20Release%2020241024.pdf}},
year = {2024},
author = {{VRM Consortium and Khronos Group}},
note = {[accessed 09-November-2025]},
date = {2024-10-24},
}

# Foundational References
@book{menache_understanding_2000,
	title = {Understanding motion capture for computer animation and video games},
	isbn = {978-0-12-490630-3},
	url = {http://archive.org/details/understandingmot0000mena_k0b1},
	abstract = {xiv, 238 p. : 24 cm; Includes index},
	pagetotal = {274},
	publisher = {San Diego, {CA} : Morgan Kaufmann},
	author = {Menache, Alberto},
	editora = {{Internet Archive}},
	editoratype = {collaborator},
	urldate = {2025-11-09},
	date = {2000},
	keywords = {Computer animation},
}


@article{wang_recent_2003,
	title = {Recent developments in human motion analysis},
	volume = {36},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320302001000},
	doi = {10.1016/S0031-3203(02)00100-0},
	abstract = {Visual analysis of human motion is currently one of the most active research topics in computer vision. This strong interest is driven by a wide spectrum of promising applications in many areas such as virtual reality, smart surveillance, perceptual interface, etc. Human motion analysis concerns the detection, tracking and recognition of people, and more generally, the understanding of human behaviors, from image sequences involving humans. This paper provides a comprehensive survey of research on computer vision based human motion analysis. The emphasis is on three major issues involved in a general human motion analysis system, namely human detection, tracking and activity understanding. Various methods for each issue are discussed in order to examine the state of the art. Finally, some research challenges and future directions are discussed.},
	pages = {585--601},
	number = {3},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Wang, Liang and Hu, Weiming and Tan, Tieniu},
	urldate = {2025-11-09},
	date = {2003-03},
	langid = {english},
}


@article{desmarais_review_2021,
	title = {A review of 3D human pose estimation algorithms for markerless motion capture},
	volume = {212},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314221001193},
	doi = {10.1016/j.cviu.2021.103275},
	abstract = {Human pose estimation is a very active research field, stimulated by its important applications in robotics, entertainment or health and sports sciences, among others. Advances in convolutional networks triggered noticeable improvements in 2D pose estimation, leading modern 3D markerless motion capture techniques to an average error per joint of 20 mm. However, with the proliferation of methods, it is becoming increasingly difficult to make an informed choice. Here, we review the leading human pose estimation methods of the past five years, focusing on metrics, benchmarks and method structures. We propose a taxonomy based on accuracy, speed and robustness that we use to classify de methods and derive directions for future research.},
	pages = {103275},
	journaltitle = {Computer Vision and Image Understanding},
	shortjournal = {Computer Vision and Image Understanding},
	author = {Desmarais, Yann and Mottet, Denis and Slangen, Pierre and Montesinos, Philippe},
	urldate = {2025-11-09},
	date = {2021-11-01},
	keywords = {3D human pose estimation, Convolutional neural networks, Survey},
}



# Online Vtuber Article 
@misc{hololive_setup,
	title = {COVER Corporation Announces the New Studio to Support Enhancements to the Virtual Streaming and Technology},
	url = {https://cover-corp.com/en/news/detail/20230511-03},
  howpublished = "\url{https://cover-corp.com/en/news/detail/20230511-03}",
  year = {2023},
  author = {{COVER Corporation}},
  note = "[accessed 09-November-2025]",
	urldate = {2025-11-09},
	date = {2023-05-11},
}

# Datasets

@inproceedings{teufelgera2025HumanOLAT,
  title = {HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis},
  author = {Timo Teufel and Pulkit Gera and Xilong Zhou and Umar Iqbal and Pramod Rao and Jan Kautz and Vladislav Golyanik and Christian Theobalt},
  year = {2025},
  booktitle={International Conference on Computer Vision (ICCV)}
}

@article{10.1371/journal.pone.0253157,
    doi = {10.1371/journal.pone.0253157},
    author = {Ghorbani, Saeed AND Mahdaviani, Kimia AND Thaler, Anne AND Kording, Konrad AND Cook, Douglas James AND Blohm, Gunnar AND Troje, Nikolaus F.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {MoVi: A large multi-purpose human motion and video dataset},
    year = {2021},
    month = {06},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pone.0253157},
    pages = {1-15},
    abstract = {Large high-quality datasets of human body shape and kinematics lay the foundation for modelling and simulation approaches in computer vision, computer graphics, and biomechanics. Creating datasets that combine naturalistic recordings with high-accuracy data about ground truth body shape and pose is challenging because different motion recording systems are either optimized for one or the other. We address this issue in our dataset by using different hardware systems to record partially overlapping information and synchronized data that lend themselves to transfer learning. This multimodal dataset contains 9 hours of optical motion capture data, 17 hours of video data from 4 different points of view recorded by stationary and hand-held cameras, and 6.6 hours of inertial measurement units data recorded from 60 female and 30 male actors performing a collection of 21 everyday actions and sports movements. The processed motion capture data is also available as realistic 3D human meshes. We anticipate use of this dataset for research on human pose estimation, action recognition, motion modelling, gait analysis, and body shape reconstruction.},
    number = {6},
}

@article{sigal2009humaneva-8bd, 
  year     = {2009}, 
  title    = {{HumanEva}: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion}, 
  author   = {Sigal, Leonid and Balan, Alexandru O. and Black, Michael J.}, 
  journal  = {International Journal of Computer Vision}, 
  issn     = {0920-5691}, 
  doi      = {10.1007/s11263-009-0273-6}, 
  abstract = {While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of competing methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting {HumanEva} datasets contain multiple subjects performing a set of predefined actions with a number of repetitions. On the order of 40,000 frames of synchronized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60 Hz with an additional 37,000 time instants of pure motion capture data. A standard set of error measures is defined for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively standard Bayesian framework with optimization in the form of Sequential Importance Resampling and Annealed Particle Filtering. In the context of this baseline algorithm we explore a variety of likelihood functions, prior models of human motion and the effects of algorithm parameters. Our experiments suggest that image observation models and motion priors play important roles in performance, and that in a multi-view laboratory environment, where initialization is available, Bayesian filtering tends to perform well. The datasets and the software are made available to the research community. This infrastructure will support the development of new articulated motion and pose estimation algorithms, will provide a baseline for the evaluation and comparison of new methods, and will help establish the current state of the art in human pose estimation and tracking.}, 
  pages    = {4}, 
  number   = {1-2}, 
  volume   = {87}
}

@conference{AMASS:ICCV:2019,
  title = {{AMASS}: Archive of Motion Capture as Surface Shapes},
  author = {Mahmood, Naureen and Ghorbani, Nima and Troje, Nikolaus F. and Pons-Moll, Gerard and Black, Michael J.},
  booktitle = {International Conference on Computer Vision},
  pages = {5442--5451},
  month = oct,
  year = {2019},
  month_numeric = {10}
}
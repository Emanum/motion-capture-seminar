@article{bassett21,
author = {Bassett, Debra},
year = {2021},
month = {02},
pages = {},
title = {Ctrl+Alt+Delete: The changing landscape of the uncanny valley and the fear of second loss},
volume = {40},
journal = {Current Psychology},
doi = {10.1007/s12144-018-0006-5}
}

@ARTICLE{9858635,
  author={Díaz-García, Lara and Reid, Andrew and Jackson-Camargo, Joseph C. and Windmill, James F. C.},
  journal={IEEE Sensors Journal}, 
  title={Toward a Bio-Inspired Acoustic Sensor: Achroia grisella’s Ear}, 
  year={2022},
  volume={22},
  number={18},
  pages={17746-17753},
  doi={10.1109/JSEN.2022.3197841}}

@misc{Hawesthoughts2023Colorblindness,
  author       = {Hawesthoughts},
  title        = {Color Blindness Wheels},
  howpublished = {Wikimedia Commons},
  month        = aug,
  year         = {2023},
  url          = {https://commons.wikimedia.org/wiki/File:Color_blindness_wheels.svg}
}

@INPROCEEDINGS{Berrezueta-Guzman2023PlagiarismDetection,
  author={Berrezueta-Guzman, Jonnathan and Paulsen, Markus and Krusche, Stephan},
  booktitle={2023 IEEE 35th International Conference on Software Engineering Education and Training (CSEE\&T)}, 
  title={Plagiarism Detection and its Effect on the Learning Outcomes}, 
  year={2023},
  volume={},
  number={},
  pages={99-108},
  keywords={Training;Plagiarism;Digital communication;Problem-solving;Engineering education;Programming profession;Information exchange;programming education;interactive learning;online training and education;software engineering education for novices;vision for education in the future},
  doi={10.1109/CSEET58097.2023.00021}}

@ARTICLE{Musick0HabitsLatex,
  author={Musick, Chad},
  journal={{ThinkSCIENCE}}, 
  title={5 Common Habits to Avoid in LaTeX}, 
  url={https://thinkscience.co.jp/en/articles/LaTeX-habits-to-avoid},
  note= {Accessed: 2025-07-09}}

@misc{Gelautz2023CVDrivingRobotics,
  author       = {Gelautz, M. and Schörkhuber, D. and Stoeva, D.},
  title        = {Computer Vision for Autonomous Driving and Robotics},
  howpublished = {Presentation, 30 Years ICG, Graz, Austria},
  year         = {2023},
  url          = {http://hdl.handle.net/20.500.12708/188254},
  note         = {Accessed: 2025-07-09}
}

# State of the Art Reviews

@ARTICLE{FromMethodstoApplicationsAReviewofDeep3DHumanMotionCapture,
  author={Niu, Zehai and Lu, Ke and Xue, Jian and Qin, Xiaoyu and Wang, Jinbao and Shao, Ling},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={From Methods to Applications: A Review of Deep 3D Human Motion Capture}, 
  year={2024},
  volume={34},
  number={11},
  pages={11340-11359},
  keywords={Motion capture;Three-dimensional displays;Pose estimation;Accuracy;Hardware;Cameras;Optical imaging;Deep learning;Pose estimation;Systematic literature review;Motion capture;deep learning;3D human pose estimation;literature review},
  doi={10.1109/TCSVT.2024.3423411}}

@Article{McInIndustryASystematicReview,
AUTHOR = {Menolotto, Matteo and Komaris, Dimitrios-Sokratis and Tedesco, Salvatore and O’Flynn, Brendan and Walsh, Michael},
TITLE = {Motion Capture Technology in Industrial Applications: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5687},
URL = {https://www.mdpi.com/1424-8220/20/19/5687},
PubMedID = {33028042},
ISSN = {1424-8220},
ABSTRACT = {The rapid technological advancements of Industry 4.0 have opened up new vectors for novel industrial processes that require advanced sensing solutions for their realization. Motion capture (MoCap) sensors, such as visual cameras and inertial measurement units (IMUs), are frequently adopted in industrial settings to support solutions in robotics, additive manufacturing, teleworking and human safety. This review synthesizes and evaluates studies investigating the use of MoCap technologies in industry-related research. A search was performed in the Embase, Scopus, Web of Science and Google Scholar. Only studies in English, from 2015 onwards, on primary and secondary industrial applications were considered. The quality of the articles was appraised with the AXIS tool. Studies were categorized based on type of used sensors, beneficiary industry sector, and type of application. Study characteristics, key methods and findings were also summarized. In total, 1682 records were identified, and 59 were included in this review. Twenty-one and 38 studies were assessed as being prone to medium and low risks of bias, respectively. Camera-based sensors and IMUs were used in 40% and 70% of the studies, respectively. Construction (30.5%), robotics (15.3%) and automotive (10.2%) were the most researched industry sectors, whilst health and safety (64.4%) and the improvement of industrial processes or products (17%) were the most targeted applications. Inertial sensors were the first choice for industrial MoCap applications. Camera-based MoCap systems performed better in robotic applications, but camera obstructions caused by workers and machinery was the most challenging issue. Advancements in machine learning algorithms have been shown to increase the capabilities of MoCap systems in applications such as activity and fatigue detection as well as tool condition monitoring and object recognition.},
DOI = {10.3390/s20195687}
}


# METHODS

@INPROCEEDINGS {PACE,
author = { Kocabas, Muhammed and Yuan, Ye and Molchanov, Pavlo and Guo, Yunrong and Black, Michael J. and Hilliges, Otmar and Kautz, Jan and Iqbal, Umar },
booktitle = { 2024 International Conference on 3D Vision (3DV) },
title = {{ PACE: Human and Camera Motion Estimation from in-the-wild Videos }},
year = {2024},
volume = {},
ISSN = {},
pages = {397-408},
abstract = { We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions. },
keywords = {Couplings;Simultaneous localization and mapping;Motion estimation;Dynamics;Cameras;Real-time systems;Trajectory},
doi = {10.1109/3DV62453.2024.00103},
url = {https://doi.ieeecomputersociety.org/10.1109/3DV62453.2024.00103},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =mar}

@INPROCEEDINGS {simpoe,
author = { Yuan, Ye and Wei, Shih-En and Simon, Tomas and Kitani, Kris and Saragih, Jason },
booktitle = { 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ SimPoE: Simulated Character Control for 3D Human Pose Estimation }},
year = {2021},
volume = {},
ISSN = {},
pages = {7155-7165},
abstract = { Accurate estimation of 3D human motion from monocular video requires modeling both kinematics (body motion without physical forces) and dynamics (motion with physical forces). To demonstrate this, we present SimPoE, a Simulation-based approach for 3D human Pose Estimation, which integrates image-based kinematic inference and physics-based dynamics modeling. SimPoE learns a policy that takes as input the current-frame pose estimate and the next image frame to control a physically-simulated character to output the next-frame pose estimate. The policy contains a learnable kinematic pose refinement unit that uses 2D keypoints to iteratively refine its kinematic pose estimate of the next frame. Based on this refined kinematic pose, the policy learns to compute dynamics-based control (e.g., joint torques) of the character to advance the current-frame pose estimate to the pose estimate of the next frame. This design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning to achieve accurate and physically-plausible pose estimation. Furthermore, we propose a meta-control mechanism that dynamically adjusts the character’s dynamics parameters based on the character state to attain more accurate pose estimates. Experiments on large-scale motion datasets demonstrate that our approach establishes the new state of the art in pose accuracy while ensuring physical plausibility. },
keywords = {Solid modeling;Computer vision;Three-dimensional displays;Pose estimation;Dynamics;Kinematics;Reinforcement learning},
doi = {10.1109/CVPR46437.2021.00708},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00708},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@INPROCEEDINGS {VIBE,
author = { Kocabas, Muhammed and Athanasiou, Nikos and Black, Michael J. },
booktitle = { 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ VIBE: Video Inference for Human Body Pose and Shape Estimation }},
year = {2020},
volume = {},
ISSN = {},
pages = {5252-5262},
abstract = { Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose "Video Inference for Body Pose and Shape Estimation'' (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. We define a novel temporal network architecture with a self-attention mechanism and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE },
keywords = {Three-dimensional displays;Shape;Training;Two dimensional displays;Pose estimation;Predictive models},
doi = {10.1109/CVPR42600.2020.00530},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.00530},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}


# Old but good
@ARTICLE{openPose,
author={Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
journal={ IEEE Transactions on Pattern Analysis \& Machine Intelligence },
title={{ OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields }},
year={2021},
volume={43},
number={01},
ISSN={1939-3539},
pages={172-186},
abstract={ Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints. },
keywords={Two dimensional displays;Pose estimation;Detectors;Runtime;Kernel;Training},
doi={10.1109/TPAMI.2019.2929257},
url = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2929257},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}


@article{chatzitofis_democap_2021,
title = {{DeMoCap}: Low-Cost Marker-Based Motion Capture},
volume = {129},
issn = {1573-1405},
url = {https://doi.org/10.1007/s11263-021-01526-z},
doi = {10.1007/s11263-021-01526-z},
abstract = {Optical marker-based motion capture ({MoCap}) remains the predominant way to acquire high-fidelity articulated body motions. We introduce {DeMoCap}, the first data-driven approach for end-to-end marker-based {MoCap}, using only a sparse setup of spatio-temporally aligned, consumer-grade infrared-depth cameras. Trading off some of their typical features, our approach is the sole robust option for far lower-cost marker-based {MoCap} than high-end solutions. We introduce an end-to-end differentiable markers-to-pose model to solve a set of challenges such as under-constrained position estimates, noisy input data and spatial configuration invariance. We simultaneously handle depth and marker detection noise, label and localize the markers, and estimate the 3D pose by introducing a novel spatial 3D coordinate regression technique under a multi-view rendering and supervision concept. {DeMoCap} is driven by a special dataset captured with 4 spatio-temporally aligned low-cost Intel {RealSense} D415 sensors and a 24 {MXT}40S camera professional {MoCap} system, used as input and ground truth, respectively.},
pages = {3338--3366},
number = {12},
journaltitle = {International Journal of Computer Vision},
shortjournal = {International Journal of Computer Vision},
author = {Chatzitofis, Anargyros and Zarpalas, Dimitrios and Daras, Petros and Kollias, Stefanos},
date = {2021-12-01},
}

@article{mehta_xnect_2020,
	title = {{XNect}: real-time multi-person 3D motion capture with a single {RGB} camera},
	volume = {39},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3386569.3392410},
	doi = {10.1145/3386569.3392410},
	shorttitle = {{XNect}},
	abstract = {We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single {RGB} camera. It operates successfully in generic scenes which may contain occlusions by objects and by other people. Our method operates in subsequent stages. The first stage is a convolutional neural network ({CNN}) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals. We contribute a new architecture for this {CNN}, called {SelecSLS} Net, that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fullyconnected neural network turns the possibly partial (on account of occlusion) 2D pose and 3D pose features for each subject into a complete 3D pose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that do not produce joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.},
	pages = {82:82:1--82:82:17},
	number = {4},
	journaltitle = {{ACM} Trans. Graph.},
	author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Elgharib, Mohamed and Fua, Pascal and Seidel, Hans-Peter and Rhodin, Helge and Pons-Moll, Gerard and Theobalt, Christian},
	urldate = {2025-10-19},
	date = {2020-08-12},
	file = {Full Text PDF:/Users/emanum/Zotero/storage/6D625ETP/Mehta et al. - 2020 - XNect real-time multi-person 3D motion capture with a single RGB camera.pdf:application/pdf},
}



# TOOLS
@article{loper_smpl_2015,
title = {{SMPL}: a skinned multi-person linear model},
volume = {34},
issn = {0730-0301},
url = {https://dl.acm.org/doi/10.1145/2816795.2818013},
doi = {10.1145/2816795.2818013},
shorttitle = {{SMPL}},
abstract = {We present a learned model of human body shape and pose-dependent shape variation that is more accurate than previous models and is compatible with existing graphics pipelines. Our Skinned Multi-Person Linear model ({SMPL}) is a skinned vertex-based model that accurately represents a wide variety of body shapes in natural human poses. The parameters of the model are learned from data including the rest pose template, blend weights, pose-dependent blend shapes, identity-dependent blend shapes, and a regressor from vertices to joint locations. Unlike previous models, the pose-dependent blend shapes are a linear function of the elements of the pose rotation matrices. This simple formulation enables training the entire model from a relatively large number of aligned 3D meshes of different people in different poses. We quantitatively evaluate variants of {SMPL} using linear or dual-quaternion blend skinning and show that both are more accurate than a Blend-{SCAPE} model trained on the same data. We also extend {SMPL} to realistically model dynamic soft-tissue deformations. Because it is based on blend skinning, {SMPL} is compatible with existing rendering engines and we make it available for research purposes.},
pages = {248:1--248:16},
number = {6},
journaltitle = {{ACM} Trans. Graph.},
author = {Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.},
urldate = {2025-10-26},
date = {2015-11-02},
}


@online{vmc_protocol_specification,
	title = {{VMC} Protocol specification},
	url = {https://protocol.vmc.info/english.html},
	abstract = {{VMCProtocol} - Easy-to-use motion capture protocol specifications for games, tools, distribution environments, etc.},
	titleaddon = {{VirtualMotionCaptureProtocol}},
	urldate = {2025-10-26},
	langid = {english},
	file = {Snapshot:/Users/emanum/Zotero/storage/ZM798N8B/english.html:text/html},
}

@misc{Mixamo, 
title   = {Mixamo}, 
url     = {https://www.mixamo.com/#/}, 
urldate = {2025-11-08}
}

@misc{VRM_consortium,
title   = {VRM Consortium},
url     = {https://vrm-consortium.org/en/},
urldate = {2025-11-09}
}

@online{vrm_press_release,
title = {The Khronos Group and VRM Consortium Collaborate to Advance
International Standardization of the VRM 3D Avatar File Format},
url = {https://vrm-consortium.org/en/common/pdf/VRMC%20Khronos%20Press%20Release%2020241024.pdf},
date = {2024-10-24},
urldate = {2025-11-09}
}
@article{bassett21,
author = {Bassett, Debra},
year = {2021},
month = {02},
pages = {},
title = {Ctrl+Alt+Delete: The changing landscape of the uncanny valley and the fear of second loss},
volume = {40},
journal = {Current Psychology},
doi = {10.1007/s12144-018-0006-5}
}

@ARTICLE{9858635,
  author={Díaz-García, Lara and Reid, Andrew and Jackson-Camargo, Joseph C. and Windmill, James F. C.},
  journal={IEEE Sensors Journal}, 
  title={Toward a Bio-Inspired Acoustic Sensor: Achroia grisella’s Ear}, 
  year={2022},
  volume={22},
  number={18},
  pages={17746-17753},
  doi={10.1109/JSEN.2022.3197841}}

@misc{Hawesthoughts2023Colorblindness,
  author       = {Hawesthoughts},
  title        = {Color Blindness Wheels},
  howpublished = {Wikimedia Commons},
  month        = aug,
  year         = {2023},
  url          = {https://commons.wikimedia.org/wiki/File:Color_blindness_wheels.svg}
}

@INPROCEEDINGS{Berrezueta-Guzman2023PlagiarismDetection,
  author={Berrezueta-Guzman, Jonnathan and Paulsen, Markus and Krusche, Stephan},
  booktitle={2023 IEEE 35th International Conference on Software Engineering Education and Training (CSEE\&T)}, 
  title={Plagiarism Detection and its Effect on the Learning Outcomes}, 
  year={2023},
  volume={},
  number={},
  pages={99-108},
  keywords={Training;Plagiarism;Digital communication;Problem-solving;Engineering education;Programming profession;Information exchange;programming education;interactive learning;online training and education;software engineering education for novices;vision for education in the future},
  doi={10.1109/CSEET58097.2023.00021}}

@ARTICLE{Musick0HabitsLatex,
  author={Musick, Chad},
  journal={{ThinkSCIENCE}}, 
  title={5 Common Habits to Avoid in LaTeX}, 
  url={https://thinkscience.co.jp/en/articles/LaTeX-habits-to-avoid},
  note= {Accessed: 2025-07-09}}

@misc{Gelautz2023CVDrivingRobotics,
  author       = {Gelautz, M. and Schörkhuber, D. and Stoeva, D.},
  title        = {Computer Vision for Autonomous Driving and Robotics},
  howpublished = {Presentation, 30 Years ICG, Graz, Austria},
  year         = {2023},
  url          = {http://hdl.handle.net/20.500.12708/188254},
  note         = {Accessed: 2025-07-09}
}

# State of the Art Reviews

@ARTICLE{FromMethodstoApplicationsAReviewofDeep3DHumanMotionCapture,
  author={Niu, Zehai and Lu, Ke and Xue, Jian and Qin, Xiaoyu and Wang, Jinbao and Shao, Ling},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={From Methods to Applications: A Review of Deep 3D Human Motion Capture}, 
  year={2024},
  volume={34},
  number={11},
  pages={11340-11359},
  keywords={Motion capture;Three-dimensional displays;Pose estimation;Accuracy;Hardware;Cameras;Optical imaging;Deep learning;Pose estimation;Systematic literature review;Motion capture;deep learning;3D human pose estimation;literature review},
  doi={10.1109/TCSVT.2024.3423411}}

@Article{McInIndustryASystematicReview,
AUTHOR = {Menolotto, Matteo and Komaris, Dimitrios-Sokratis and Tedesco, Salvatore and O’Flynn, Brendan and Walsh, Michael},
TITLE = {Motion Capture Technology in Industrial Applications: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5687},
URL = {https://www.mdpi.com/1424-8220/20/19/5687},
PubMedID = {33028042},
ISSN = {1424-8220},
ABSTRACT = {The rapid technological advancements of Industry 4.0 have opened up new vectors for novel industrial processes that require advanced sensing solutions for their realization. Motion capture (MoCap) sensors, such as visual cameras and inertial measurement units (IMUs), are frequently adopted in industrial settings to support solutions in robotics, additive manufacturing, teleworking and human safety. This review synthesizes and evaluates studies investigating the use of MoCap technologies in industry-related research. A search was performed in the Embase, Scopus, Web of Science and Google Scholar. Only studies in English, from 2015 onwards, on primary and secondary industrial applications were considered. The quality of the articles was appraised with the AXIS tool. Studies were categorized based on type of used sensors, beneficiary industry sector, and type of application. Study characteristics, key methods and findings were also summarized. In total, 1682 records were identified, and 59 were included in this review. Twenty-one and 38 studies were assessed as being prone to medium and low risks of bias, respectively. Camera-based sensors and IMUs were used in 40% and 70% of the studies, respectively. Construction (30.5%), robotics (15.3%) and automotive (10.2%) were the most researched industry sectors, whilst health and safety (64.4%) and the improvement of industrial processes or products (17%) were the most targeted applications. Inertial sensors were the first choice for industrial MoCap applications. Camera-based MoCap systems performed better in robotic applications, but camera obstructions caused by workers and machinery was the most challenging issue. Advancements in machine learning algorithms have been shown to increase the capabilities of MoCap systems in applications such as activity and fatigue detection as well as tool condition monitoring and object recognition.},
DOI = {10.3390/s20195687}
}


# METHODS

@INPROCEEDINGS {PACE,
author = { Kocabas, Muhammed and Yuan, Ye and Molchanov, Pavlo and Guo, Yunrong and Black, Michael J. and Hilliges, Otmar and Kautz, Jan and Iqbal, Umar },
booktitle = { 2024 International Conference on 3D Vision (3DV) },
title = {{ PACE: Human and Camera Motion Estimation from in-the-wild Videos }},
year = {2024},
volume = {},
ISSN = {},
pages = {397-408},
abstract = { We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions. },
keywords = {Couplings;Simultaneous localization and mapping;Motion estimation;Dynamics;Cameras;Real-time systems;Trajectory},
doi = {10.1109/3DV62453.2024.00103},
url = {https://doi.ieeecomputersociety.org/10.1109/3DV62453.2024.00103},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =mar}

@inproceedings{FLAG,
  location = {New Orleans, {LA}, {USA}},
  title = {{FLAG}: Flow-based 3D Avatar Generation from Sparse Observations},
  rights = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  url = {https://ieeexplore.ieee.org/document/9879257/},
  doi = {10.1109/CVPR52688.2022.01290},
  shorttitle = {{FLAG}},
  abstract = {To represent people in mixed reality applications for collaboration and communication, we need to generate realistic and faithful avatar poses. However, the signal streams that can be applied for this task from head-mounted devices ({HMDs}) are typically limited to head pose and hand pose estimates. While these signals are valuable, they are an incomplete representation of the human body, making it challenging to generate a faithful full-body avatar. We address this challenge by developing a ﬂow-based generative model of the 3D human body from sparse observations, wherein we learn not only a conditional distribution of 3D human pose, but also a probabilistic mapping from observations to the latent space from which we can generate a plausible pose along with uncertainty estimates for the joints. We show that our approach is not only a strong predictive model, but can also act as an efﬁcient pose prior in different optimization settings where a good initial latent code plays a major role.},
  eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  pages = {13243--13252},
  booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher = {{IEEE}},
  author = {Aliakbarian, Sadegh and Cameron, Pashmina and Bogo, Federica and Fitzgibbon, Andrew and Cashman, Thomas J.},
  urldate = {2025-11-09},
  date = {2022-06},
  year = {2022},
  langid = {english},
}


@incollection{leonardis_avatarpose_2025,
	location = {Cham},
	title = {{AvatarPose}: Avatar-Guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos},
	volume = {15111},
	isbn = {978-3-031-73667-4 978-3-031-73668-1},
	url = {https://link.springer.com/10.1007/978-3-031-73668-1_13},
	shorttitle = {{AvatarPose}},
	abstract = {Despite progress in human motion capture, existing multiview methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of avatars to add penetration constraints. Moreover, both 3D poses and avatars are optimized in an alternating manner. Our experimental results demonstrate state-of-the-art performance on several public datasets.},
	pages = {215--233},
	booktitle = {Computer Vision – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Lu, Feichi and Dong, Zijian and Song, Jie and Hilliges, Otmar},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	urldate = {2025-11-09},
	date = {2025},
	langid = {english},
	doi = {10.1007/978-3-031-73668-1_13},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {PDF:/Users/emanum/Zotero/storage/9MISYHNV/Lu et al. - 2025 - AvatarPose Avatar-Guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Video.pdf:application/pdf},
}


@INPROCEEDINGS {simpoe,
author = { Yuan, Ye and Wei, Shih-En and Simon, Tomas and Kitani, Kris and Saragih, Jason },
booktitle = { 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ SimPoE: Simulated Character Control for 3D Human Pose Estimation }},
year = {2021},
volume = {},
ISSN = {},
pages = {7155-7165},
abstract = { Accurate estimation of 3D human motion from monocular video requires modeling both kinematics (body motion without physical forces) and dynamics (motion with physical forces). To demonstrate this, we present SimPoE, a Simulation-based approach for 3D human Pose Estimation, which integrates image-based kinematic inference and physics-based dynamics modeling. SimPoE learns a policy that takes as input the current-frame pose estimate and the next image frame to control a physically-simulated character to output the next-frame pose estimate. The policy contains a learnable kinematic pose refinement unit that uses 2D keypoints to iteratively refine its kinematic pose estimate of the next frame. Based on this refined kinematic pose, the policy learns to compute dynamics-based control (e.g., joint torques) of the character to advance the current-frame pose estimate to the pose estimate of the next frame. This design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning to achieve accurate and physically-plausible pose estimation. Furthermore, we propose a meta-control mechanism that dynamically adjusts the character’s dynamics parameters based on the character state to attain more accurate pose estimates. Experiments on large-scale motion datasets demonstrate that our approach establishes the new state of the art in pose accuracy while ensuring physical plausibility. },
keywords = {Solid modeling;Computer vision;Three-dimensional displays;Pose estimation;Dynamics;Kinematics;Reinforcement learning},
doi = {10.1109/CVPR46437.2021.00708},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00708},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@INPROCEEDINGS {VIBE,
author = { Kocabas, Muhammed and Athanasiou, Nikos and Black, Michael J. },
booktitle = { 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ VIBE: Video Inference for Human Body Pose and Shape Estimation }},
year = {2020},
volume = {},
ISSN = {},
pages = {5252-5262},
abstract = { Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose "Video Inference for Body Pose and Shape Estimation'' (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. We define a novel temporal network architecture with a self-attention mechanism and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE },
keywords = {Three-dimensional displays;Shape;Training;Two dimensional displays;Pose estimation;Predictive models},
doi = {10.1109/CVPR42600.2020.00530},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.00530},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}


# OPENPOSE START

@article{openPose,
  author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  year = {2019}
}

@inproceedings{openPose_2_simon2017hand,
  author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},
  year = {2017}
}

@inproceedings{openPose_3_cao2017realtime,
  author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  year = {2017}
}

@inproceedings{openPose_4_wei2016cpm,
  author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Convolutional pose machines},
  year = {2016}
}

@misc{openPose_github,
  author = {Zhe Cao and Gines Hidalgo Martinez and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  howpublished = {\url{https://github.com/CMU-Perceptual-Computing-Lab/openpose}},
  year = {2019}
}
# OPENPOSE END

@misc{mocopi,
author = {{Sony Interactive Entertainment Inc.}},
title = {mocopi},
howpublished = {\url{https://www.sony.co.jp/en/Products/mocopi-dev/en/}},
year = {2023},
note = {[accessed 25-November-2025]},
}


@article{chatzitofis2021democap,
  title={DeMoCap: Low-Cost Marker-Based Motion Capture},
  author={Chatzitofis, Anargyros and Zarpalas, Dimitrios and Daras, Petros and Kollias, Stefanos},
  journal={International Journal of Computer Vision},
  volume={129},
  number={12},
  pages={3338--3366},
  year={2021},
  publisher={Springer}
}

@article{mehta_xnect_2020,
author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Elgharib, Mohamed and Fua, Pascal and Seidel, Hans-Peter and Rhodin, Helge and Pons-Moll, Gerard and Theobalt, Christian},
title = {XNect: real-time multi-person 3D motion capture with a single RGB camera},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3386569.3392410},
doi = {10.1145/3386569.3392410},
abstract = {We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single RGB camera. It operates successfully in generic scenes which may contain occlusions by objects and by other people. Our method operates in subsequent stages. The first stage is a convolutional neural network (CNN) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals. We contribute a new architecture for this CNN, called SelecSLS Net, that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fullyconnected neural network turns the possibly partial (on account of occlusion) 2D pose and 3D pose features for each subject into a complete 3D pose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that do not produce joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.},
journal = {ACM Trans. Graph.},
month = aug,
articleno = {82},
numpages = {17},
keywords = {RGB, human body pose, monocular, motion capture, real-time}
}

@inproceedings{jiang2024egoposer,
  title={EgoPoser: Robust real-time egocentric pose estimation from sparse and intermittent observations everywhere},
  author={Jiang, Jiaxi and Streli, Paul and Meier, Manuel and Holz, Christian},
  booktitle={European Conference on Computer Vision},
  year={2024},
  organization={Springer}
}	


# TOOLS
@article{loper_smpl_2015,
title = {{SMPL}: a skinned multi-person linear model},
volume = {34},
issn = {0730-0301},
url = {https://dl.acm.org/doi/10.1145/2816795.2818013},
doi = {10.1145/2816795.2818013},
shorttitle = {{SMPL}},
abstract = {We present a learned model of human body shape and pose-dependent shape variation that is more accurate than previous models and is compatible with existing graphics pipelines. Our Skinned Multi-Person Linear model ({SMPL}) is a skinned vertex-based model that accurately represents a wide variety of body shapes in natural human poses. The parameters of the model are learned from data including the rest pose template, blend weights, pose-dependent blend shapes, identity-dependent blend shapes, and a regressor from vertices to joint locations. Unlike previous models, the pose-dependent blend shapes are a linear function of the elements of the pose rotation matrices. This simple formulation enables training the entire model from a relatively large number of aligned 3D meshes of different people in different poses. We quantitatively evaluate variants of {SMPL} using linear or dual-quaternion blend skinning and show that both are more accurate than a Blend-{SCAPE} model trained on the same data. We also extend {SMPL} to realistically model dynamic soft-tissue deformations. Because it is based on blend skinning, {SMPL} is compatible with existing rendering engines and we make it available for research purposes.},
pages = {248:1--248:16},
number = {6},
journaltitle = {{ACM} Trans. Graph.},
author = {Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.},
urldate = {2025-10-26},
date = {2015-11-02},
}

@inproceedings{SMPLX,
  title = {Expressive Body Capture: {3D} Hands, Face, and Body from a Single Image},
  author = {Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed A. A. and Tzionas, Dimitrios and Black, Michael J.},
  booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {10975--10985},
  year = {2019}
}


@misc{vmc_protocol_specification,
	title = {{VMC} Protocol specification},
  howpublished = {\url{https://protocol.vmc.info/english.html}},
  year = {2025},
  note = {[accessed 09-November-2025]},
  author = {{VMCProtocol}},
	abstract = {{VMCProtocol} - Easy-to-use motion capture protocol specifications for games, tools, distribution environments, etc.},
	urldate = {2025-10-26},
}

@misc{Mixamo, 
title   = {Mixamo}, 
author  = {{Adobe}},
url     = {https://www.mixamo.com/#/}, 
urldate = {2025-11-08}
}

@misc{VRM_consortium,
title   = {VRM Consortium},
url     = {https://vrm-consortium.org/en/},
howpublished = {\url{https://vrm-consortium.org/en/}},
year = {2025},
author = {{VRM Consortium}},
note = {[accessed 09-November-2025]},
urldate = {2025-11-09}
}

@misc{vrm_press_release,
title = {The Khronos Group and VRM Consortium Collaborate to Advance
International Standardization of the VRM 3D Avatar File Format},
howpublished = {\url{https://vrm-consortium.org/en/common/pdf/VRMC%20Khronos%20Press%20Release%2020241024.pdf}},
year = {2024},
author = {{VRM Consortium and Khronos Group}},
note = {[accessed 09-November-2025]},
date = {2024-10-24},
}

# Foundational References
@book{menache_understanding_2000,
	title = {Understanding motion capture for computer animation and video games},
	isbn = {978-0-12-490630-3},
	url = {http://archive.org/details/understandingmot0000mena_k0b1},
	abstract = {xiv, 238 p. : 24 cm; Includes index},
	pagetotal = {274},
	publisher = {San Diego, {CA} : Morgan Kaufmann},
	author = {Menache, Alberto},
	editora = {{Internet Archive}},
	editoratype = {collaborator},
	urldate = {2025-11-09},
	date = {2000},
	keywords = {Computer animation},
}


@article{wang_recent_2003,
	title = {Recent developments in human motion analysis},
	volume = {36},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320302001000},
	doi = {10.1016/S0031-3203(02)00100-0},
	abstract = {Visual analysis of human motion is currently one of the most active research topics in computer vision. This strong interest is driven by a wide spectrum of promising applications in many areas such as virtual reality, smart surveillance, perceptual interface, etc. Human motion analysis concerns the detection, tracking and recognition of people, and more generally, the understanding of human behaviors, from image sequences involving humans. This paper provides a comprehensive survey of research on computer vision based human motion analysis. The emphasis is on three major issues involved in a general human motion analysis system, namely human detection, tracking and activity understanding. Various methods for each issue are discussed in order to examine the state of the art. Finally, some research challenges and future directions are discussed.},
	pages = {585--601},
	number = {3},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Wang, Liang and Hu, Weiming and Tan, Tieniu},
	urldate = {2025-11-09},
	date = {2003-03},
	langid = {english},
}


@article{desmarais_review_2021,
	title = {A review of 3D human pose estimation algorithms for markerless motion capture},
	volume = {212},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314221001193},
	doi = {10.1016/j.cviu.2021.103275},
	abstract = {Human pose estimation is a very active research field, stimulated by its important applications in robotics, entertainment or health and sports sciences, among others. Advances in convolutional networks triggered noticeable improvements in 2D pose estimation, leading modern 3D markerless motion capture techniques to an average error per joint of 20 mm. However, with the proliferation of methods, it is becoming increasingly difficult to make an informed choice. Here, we review the leading human pose estimation methods of the past five years, focusing on metrics, benchmarks and method structures. We propose a taxonomy based on accuracy, speed and robustness that we use to classify de methods and derive directions for future research.},
	pages = {103275},
	journaltitle = {Computer Vision and Image Understanding},
	shortjournal = {Computer Vision and Image Understanding},
	author = {Desmarais, Yann and Mottet, Denis and Slangen, Pierre and Montesinos, Philippe},
	urldate = {2025-11-09},
	date = {2021-11-01},
	keywords = {3D human pose estimation, Convolutional neural networks, Survey},
}



# Online Vtuber Article 
@misc{hololive_setup,
	title = {COVER Corporation Announces the New Studio to Support Enhancements to the Virtual Streaming and Technology},
	url = {https://cover-corp.com/en/news/detail/20230511-03},
  howpublished = "\url{https://cover-corp.com/en/news/detail/20230511-03}",
  year = {2023},
  author = {{COVER Corporation}},
  note = "[accessed 09-November-2025]",
	urldate = {2025-11-09},
	date = {2023-05-11},
}


@misc{VICON,
author = {{Vicon Motion Systems Ltd.}},
title = {Vicon Motion Capture Systems},
howpublished = {\url{https://www.vicon.com/}},
year = {2025},
note = {[accessed 25-November-2025]},
}

# Datasets

@inproceedings{teufelgera2025HumanOLAT,
  title = {HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis},
  author = {Timo Teufel and Pulkit Gera and Xilong Zhou and Umar Iqbal and Pramod Rao and Jan Kautz and Vladislav Golyanik and Christian Theobalt},
  year = {2025},
  booktitle={International Conference on Computer Vision (ICCV)}
}

@article{10.1371/journal.pone.0253157,
    doi = {10.1371/journal.pone.0253157},
    author = {Ghorbani, Saeed AND Mahdaviani, Kimia AND Thaler, Anne AND Kording, Konrad AND Cook, Douglas James AND Blohm, Gunnar AND Troje, Nikolaus F.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {MoVi: A large multi-purpose human motion and video dataset},
    year = {2021},
    month = {06},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pone.0253157},
    pages = {1-15},
    abstract = {Large high-quality datasets of human body shape and kinematics lay the foundation for modelling and simulation approaches in computer vision, computer graphics, and biomechanics. Creating datasets that combine naturalistic recordings with high-accuracy data about ground truth body shape and pose is challenging because different motion recording systems are either optimized for one or the other. We address this issue in our dataset by using different hardware systems to record partially overlapping information and synchronized data that lend themselves to transfer learning. This multimodal dataset contains 9 hours of optical motion capture data, 17 hours of video data from 4 different points of view recorded by stationary and hand-held cameras, and 6.6 hours of inertial measurement units data recorded from 60 female and 30 male actors performing a collection of 21 everyday actions and sports movements. The processed motion capture data is also available as realistic 3D human meshes. We anticipate use of this dataset for research on human pose estimation, action recognition, motion modelling, gait analysis, and body shape reconstruction.},
    number = {6},
}

@article{sigal2009humaneva-8bd, 
  year     = {2009}, 
  title    = {{HumanEva}: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion}, 
  author   = {Sigal, Leonid and Balan, Alexandru O. and Black, Michael J.}, 
  journal  = {International Journal of Computer Vision}, 
  issn     = {0920-5691}, 
  doi      = {10.1007/s11263-009-0273-6}, 
  abstract = {While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of competing methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting {HumanEva} datasets contain multiple subjects performing a set of predefined actions with a number of repetitions. On the order of 40,000 frames of synchronized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60 Hz with an additional 37,000 time instants of pure motion capture data. A standard set of error measures is defined for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively standard Bayesian framework with optimization in the form of Sequential Importance Resampling and Annealed Particle Filtering. In the context of this baseline algorithm we explore a variety of likelihood functions, prior models of human motion and the effects of algorithm parameters. Our experiments suggest that image observation models and motion priors play important roles in performance, and that in a multi-view laboratory environment, where initialization is available, Bayesian filtering tends to perform well. The datasets and the software are made available to the research community. This infrastructure will support the development of new articulated motion and pose estimation algorithms, will provide a baseline for the evaluation and comparison of new methods, and will help establish the current state of the art in human pose estimation and tracking.}, 
  pages    = {4}, 
  number   = {1-2}, 
  volume   = {87}
}

@conference{AMASS:ICCV:2019,
  title = {{AMASS}: Archive of Motion Capture as Surface Shapes},
  author = {Mahmood, Naureen and Ghorbani, Nima and Troje, Nikolaus F. and Pons-Moll, Gerard and Black, Michael J.},
  booktitle = {International Conference on Computer Vision},
  pages = {5442--5451},
  month = oct,
  year = {2019},
  month_numeric = {10}
}

@misc{sfudataset,
  title = {SFU Motion Capture Database},
  author = {Simon Fraser University, National University of Singapore},
  year = {2011},
  howpublished = "\url{http://mocap.cs.sfu.ca/} ",
}


@article{Joo_2017_TPAMI,
  title={Panoptic Studio: A Massively Multiview System for Social Interaction Capture},
  author={Joo, Hanbyul and Simon, Tomas and Li, Xulong and Liu, Hao and Tan, Lei and Gui, Lin and Banerjee, Sean and Godisart, Timothy Scott and Nabbe, Bart and Matthews, Iain and Kanade, Takeo and Nobuhara, Shohei and Sheikh, Yaser},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017}
}

@InProceedings{Joo_2015_ICCV,
title = {Panoptic Studio: A Massively Multiview System for Social Motion Capture},
author = {Joo, Hanbyul and Liu, Hao and Tan, Lei and Gui, Lin and Nabbe, Bart and Matthews, Iain and Kanade, Takeo and Nobuhara, Shohei and Sheikh, Yaser},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
year = {2015}
}

@inproceedings{singleshotmultiperson2018,
title = {Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB},
author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Sridhar, Srinath and Pons-Moll, Gerard and Theobalt, Christian},
booktitle = {3D Vision (3DV), 2018 Sixth International Conference on},
month = {sep},
year = {2018},
organization={IEEE},
url = {http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson}
}  

@ARTICLE{shi2024tro,
  author={Shi, Chenghao and Chen, Xieyuanli and Xiao, Junhao and Dai, Bin and Lu, Huimin},
  journal={IEEE Transactions on Robotics}, 
  title={Fast and Accurate Deep Loop Closing and Relocalization for Reliable LiDAR SLAM}, 
  year={2024},
  volume={40},
  number={},
  pages={2620-2640},
  doi={10.1109/TRO.2024.3386363}}

@INPROCEEDINGS {mp3d,
author = { Dabral, Rishabh and Gundavarapu, Nitesh B and Mitra, Rahul and Sharma, Abhishek and Ramakrishnan, Ganesh and Jain, Arjun },
booktitle = { 2019 International Conference on 3D Vision (3DV) },
title = {{ Multi-Person 3D Human Pose Estimation from Monocular Images }},
year = {2019},
volume = {},
ISSN = {},
pages = {405-414},
abstract = { Multi-person 3D human pose estimation from a single image is a challenging problem, especially for in-the-wild settings due to the lack of 3D annotated data. We propose HG-RCNN, a Mask-RCNN based network that also leverages the benefits of the Hourglass architecture for multi-person 3D Human Pose Estimation. A two-staged approach is presented that first estimates the 2D keypoints in every Region of Interest (RoI) and then lifts the estimated keypoints to 3D. Finally, the estimated 3D poses are placed in camera-coordinates using weak-perspective projection assumption and joint optimization of focal length and root translations. The result is a simple and modular network for multi-person 3D human pose estimation that does not require any multi-person 3D pose dataset. Despite its simple formulation, HG-RCNN achieves the state-of-the-art results on MuPoTS-3D while also approximating the 3D pose in the camera-coordinate system. },
keywords = {Three-dimensional displays;Pose estimation;Two dimensional displays;Heating systems;Pipelines;Cameras;Cognition},
doi = {10.1109/3DV.2019.00052},
url = {https://doi.ieeecomputersociety.org/10.1109/3DV.2019.00052},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =sep}

@ARTICLE{lcrnetplusplus,
  author={Rogez, Grégory and Weinzaepfel, Philippe and Schmid, Cordelia},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={LCR-Net++: Multi-Person 2D and 3D Pose Detection in Natural Images}, 
  year={2020},
  volume={42},
  number={5},
  pages={1146-1161},
  keywords={Three-dimensional displays;Two dimensional displays;Pose estimation;Proposals;Joints;Heating systems;Training data;Human 3D pose estimation;2D pose estimation;detection;localization;classification;regression;CNN},
  doi={10.1109/TPAMI.2019.2892985}}



@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}}


@inproceedings{UltraInertialPoser,
  author = {Armani, Rayan and Qian, Changlin and Jiang, Jiaxi and Holz, Christian},
  title = {{Ultra Inertial Poser}: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging},
  year = {2024},
  isbn = {9798400705250},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3641519.3657465},
  doi = {10.1145/3641519.3657465},
  booktitle = {ACM SIGGRAPH 2024 Conference Papers},
  articleno = {51},
  numpages = {11},
  keywords = {Human pose estimation, IMU, UWB, sparse tracking},
  location = {Denver, CO, USA},
  series = {SIGGRAPH '24}
}


@misc{UltraInertialPoser_YT,
  author = {Armani, Rayan and Qian, Changlin and Jiang, Jiaxi and Holz, Christian},
  title = {{Ultra Inertial Poser}: Video Presentation},
  year = {2024},
  author = {{Ultra Inertial Poser}: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging},
  howpublished = {\url{https://www.youtube.com/watch?v=CMT2ixXq0HA}},
}


@article{TransPose,
author = {Yi, Xinyu and Zhou, Yuxiao and Xu, Feng},
title = {TransPose: real-time 3D human translation and pose estimation with six inertial sensors},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459786},
doi = {10.1145/3450626.3459786},
abstract = {Motion capture is facing some new possibilities brought by the inertial sensing technologies which do not suffer from occlusion or wide-range recordings as vision-based solutions do. However, as the recorded signals are sparse and quite noisy, online performance and global translation estimation turn out to be two key difficulties. In this paper, we present TransPose, a DNN-based approach to perform full motion capture (with both global translations and body poses) from only 6 Inertial Measurement Units (IMUs) at over 90 fps. For body pose estimation, we propose a multi-stage network that estimates leaf-to-full joint positions as intermediate results. This design makes the pose estimation much easier, and thus achieves both better accuracy and lower computation cost. For global translation estimation, we propose a supporting-foot-based method and an RNN-based method to robustly solve for the global translations with a confidence-based fusion technique. Quantitative and qualitative comparisons show that our method outperforms the state-of-the-art learning- and optimization-based methods with a large margin in both accuracy and efficiency. As a purely inertial sensor-based approach, our method is not limited by environmental settings (e.g., fixed cameras), making the capture free from common difficulties such as wide-range motion space and strong occlusion.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {86},
numpages = {13},
keywords = {real-time, pose estimation, inverse kinematics, RNN, IMU}
}

@article{EgoLocate2023,
    author = {Yi, Xinyu and Zhou, Yuxiao and Habermann, Marc and Golyanik, Vladislav and Pan, Shaohua and Theobalt, Christian and Xu, Feng},
    title = {EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors},
    journal={ACM Transactions on Graphics (TOG)},
    year = {2023},
    volume = {42},
    number = {4},
    numpages = {17},
    articleno = {76},
    publisher = {ACM}
}

@InProceedings{PIPCVPR2022,
  author = {Yi, Xinyu and Zhou, Yuxiao and Habermann, Marc and Shimada, Soshi and Golyanik, Vladislav and Theobalt, Christian and Xu, Feng},
  title = {Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2022}
}

# UWB

@Article{s19204466,
AUTHOR = {Ledergerber, Anton and D’Andrea, Raffaello},
TITLE = {Ultra-Wideband Angle of Arrival Estimation Based on Angle-Dependent Antenna Transfer Function},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4466},
URL = {https://www.mdpi.com/1424-8220/19/20/4466},
PubMedID = {31618897},
ISSN = {1424-8220},
ABSTRACT = {Ultra-wideband radio signals are used in communication, indoor localization and radar systems, due to the high data rates, the high resilience to fading and the fine temporal resolution that can be achieved with a large bandwidth. This paper introduces a new method to estimate the angle of arrival of ultra-wideband radio signals with which existing time-of-flight based localization and radar systems can be augmented at no additional hardware cost. The method does not require multiple transmitter or receiver antennas, or relative motion between transmitter and receiver. Instead, it is solely based on the angle-dependent impulse response function of ultra-wideband antennas. Datasets on which the method is evaluated are publicly available. The method is further applied to a localization problem and it is shown how a robot can self-localize solely based on these angle of arrival estimates, and how they can be combined with time-of-flight measurements. Even though existing angle of arrival techniques that use multiple antennas show better accuracy, the method presented herein looks promising enough to be developed further and could potentially lead to electronically and mechanically simpler angle of arrival estimation technology.},
DOI = {10.3390/s19204466}
}


@article{cuevas2025mamma,
  title={MAMMA: Markerless \& Automatic Multi-Person Motion Action Capture},
  author={Cuevas-Velasquez, Hanz and Yiannakidis, Anastasios and Shin, Soyong and Becherini, Giorgio and H{\"o}schle, Markus and Tesch, Joachim and Obersat, Taylor and Alexiadis, Tsvetelina and Black, Michael J},
  journal={arXiv preprint arXiv:2506.13040},
  year={2025}
}

\section{Discussion}
\label{chapter_discussion}

\open{
While you don't have to include a discussion section, it is encouraged that you try to add to existing knowledge in some way. This may be through comparison of existing methods, critical reflection, etc.}

% Modal based constraints: 


% we fit a model-based skeleton to the 3D and 2D predictions in order
% to satisfy kinematic constraints and reconcile the 2D and 3D predic-
% tions across time -> XNect


% Below is a table positioned exactly here:\\

\noindent\makebox[\textwidth]{%
\begin{tabularx}{1.4\textwidth} { 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X | }
 \hline
 \textbf{Approach} & \textbf{Sensor Setup} & \textbf{Output} & \textbf{Time complexity} & \textbf{Key Limitations} \\
 \hline
 \textbf{OpenPose} 2021 \cite{openPose} (\ref{method_openpose}) & markerless \newline single RGB camera for 2D \break multi-view RGB cameras for 3D & multi person 2D keypoints including foot and hands. Option for single person 3D keypoints via triangulation in a multi-view setup & realtime & 3D capabilities very limited; Body occlusion and high crowded images lead to false keypoints. \\
 \hline
 \textbf{DeMoCap} 2021 \cite{chatzitofis2021democap} (\ref{method_democap}) & 53 body markers \newline  + 4 low-cost depth cameras  & single person 3D pose in SMPL & realtime & limited capture space due to 4m range limit of sensors \newline required 53 markers or retraining \\
 \hline
 \textbf{XNect} 2020 \cite{mehta_xnect_2020} (\ref{method_xnect}) & single RGB camera & multi person 3D pose as Skeleton & realtime & Occlusion of neck leads to complete loss of person; Issues with complex interactions; person mismatch between frames \\
 \hline
 \textbf{Ultra Inertial Poser} 2024 \cite{UltraInertialPoser} (\ref{method_uip}) &  6 wearable trackers with IMU + UWB & single person 3D pose in SMPL & realtime & clear visible jitter artifacts; low tracking position accuracy (order of 5-11 cm)  \\
 \hline
 \textbf{MAMMA} 2025 \cite{cuevas2025mamma} (\ref{method_mamma}) & 33 high quality RGB cameras & multi person 3D pose in SMPL-X & offline; processing duration around 3x than recording & high number of camera required; low mobility \\
 \hline
 \textbf{SimPoE} 2021 \cite{simpoe} & single RGB camera & single person 3D pose (compatible with body mesh models like SMPL)& realtime & requires 3D scene modeling; no comparison to multi-view approaches; no interaction / complex scenes  \\
 \hline
 \textbf{Avatarpose} 2024 \cite{leonardis_avatarpose_2025} & sparse set of RGB cameras & multi person 3D pose in SMPL & offline & no hand tracking; issues when initial pose is in opposite direction  \\
 \hline
\end{tabularx}%
}


\noindent\makebox[\textwidth]{%
\begin{tabularx}{1.4\textwidth} { 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X | }
 \hline
 \textbf{Approach} & \textbf{Sensor Setup} & \textbf{Output} & \textbf{Time complexity} & \textbf{Key Limitations} \\
 \hline
 \textbf{PACE} 2024 \cite{PACE} & single moveable RGB camera in the wild & multi person 3D pose in SMPL and global splace & offline & assumption of planar ground; fails at similar scenarios where SLAM methods fail like similar backgrounds\\
 \hline
%  \textbf{EasyMocap} \cite{easymocap} \cite{shuai2022multinb} \cite{lin2022efficient} \cite{dong2021fast} \cite{dong2020motion} \cite{peng2021neural} \cite{fang2021mirrored} & single or multi view RGB camers & multi person 3D pose in SMPL & & \\
%  \hline
% \textbf{EgoPoser} 2024 \cite{jiang2024egoposer}  & egocentric wide angle RGB camera from HMD devices & single person 3D pose in SMPL & realtime & \\
%  \hline
%  \textbf{FLAG} 2022 \cite{FLAG} & & & & \\
%  \hline
%  \textbf{EgoPoser} \cite{jiang2024egoposer} & & & & worse than SimPoe\\
%  \hline
%  \textbf{TransPose} 2021 \cite{TransPose} & 6 wearable IMU sensors & & realtime & \\
%  \hline
%  \textbf{EgoLocate} \cite{EgoLocate2023} & & & & \\
%  \hline
%  \textbf{Sony Mocopi} \cite{mocopi} & & & & \\
%  \hline
%  \textbf{Physical Inertial Poser (PIP)} \cite{PIPCVPR2022} & & & & \\
% \hline
\end{tabularx}%
}


% \pagebreak
% \thispagestyle{empty}
% \begin{landscape}

% \centering
% \renewcommand{\arraystretch}{1.5}
% \begin{tabularx}{\linewidth} { 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X 
%   | >{\centering\arraybackslash}X | }
%  \hline
%  \textbf{Dataset} & \textbf{OpenPose} & \textbf{DeMoCap} & \textbf{XNect} & \textbf{Ultra Inertial Poser} & \textbf{MAMMA} & \textbf{SimPoE} & \textbf{Avatarpose} & \textbf{PACE} \\
%  \hline
%  COCO &  &  &  &  &  &  &  &  \\
%  \hline
%  MPII &  &  &  &  &  &  &  &  \\
%  \hline
%  SFU Dataset &  &  &  &  &  &  &  &  \\
%  \hline
%  MuPoTS-3D &  &  &  &  &  &  &  &  \\
%  \hline
%  Panoptic &  &  &  &  &  &  &  &  \\
%  \hline
%  Hi4D &  &  &  &  &  &  &  &  \\
%  \hline
%  Harmony4D &  &  &  &  &  &  &  &  \\
%  \hline
%  RICH &  &  &  &  &  &  &  &  \\
%  \hline
%  AMASS &  &  &  &  &  &  &  &  \\
%  \hline
%  Synthetic &  &  &  &  &  &  &  &  \\
%  \hline
%  Proprietary &  &  &  &  &  &  &  &  \\
%  \hline
% \end{tabularx}

% \vspace{0.5cm}
% \textbf{Table 2:} Comparison of motion capture approaches and their evaluation datasets. This table shows which datasets are used for training and evaluation by each approach.

% \end{landscape}

% \pagebreak
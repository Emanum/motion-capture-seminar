\section{Discussion}
\label{chapter_discussion}

In Table \hyperref[tab:comparison]{1}, we summarized the approaches from Chapter \ref{chapter_methods} and compared them to each other based on the sensor setup, their output data, time complexity and their key limitations. 
Furthermore, 3 more interesting approaches SimPoE \cite{simpoe} PACE \cite{PACE} and Avatarpose \cite{leonardis_avatarpose_2025} were added that was not discussed in details before. \\
Table \hyperref[tab:dataset]{2} compares the same approaches in terms of tracking accuracy (MPJPE).\\\\
Based on the requirements for our task set in Chapter \ref{chapter_task} we are evaluating which kind of approach could be fitting.
After examining video examples, we assume that optical camera based systems are preferred due to lower susceptibility to jitter and drift errors compared to IMU sensors as seen in Ultra Inertial Poser \cite{UltraInertialPoser}. 
The limitations in distance and refresh for depth sensors as seen in DeMoCap \cite{chatzitofis2021democap} further suggest the usage of optical cameras.
Furthermore, in order to reduce occlusion errors and capture close human interaction as good as possible using multiple camera angles seems unavoidable as the results of XNect \ref{method_xnect} and other single RGB camera approaches like SimPoE \cite{simpoe} or PACE \cite{PACE} show us. 
In order to use existing 3D models during a live show support for a common file format already used in the industry based on mesh data such as VRM or FBX is unavoidable.
While SMPL(X) offers great tracking potential we do not see the need for it in our use-case. A method with VMC support would solve a lot of issues for integrating new motion capture tools to existing workflow pipelines.
Methods designed to work in the wild like PACE \cite{PACE} or EgoPoser \cite{jiang2024egoposer} are impressive on its own but seem to be more suited for wearable AR/VR devices than multi-person tracking in a semi portable studio setup.\\
The combination of multi person tracking during a real-time scenarios still serves as a great challenge as our literature review shows. From the 8 approaches in Table \hyperref[tab:comparison]{1} only XNect \ref{method_xnect} fulfilled both of these requirements.
However, if we ignore the real-time aspect MAMMA \cite{cuevas2025mamma} and Avatarpose \cite{leonardis_avatarpose_2025} seems to be the best approaches for our use-case. 
In their current setup MAMMA focus on tracking accuracy and applies 33 cameras which is considerable higher than other methods that work in real-time.
Therefore, we hope that we will see upcoming approaches with focusing on real-time instead of tracking accuracy while still allowing to track multiple people.



% TABLE --------------
\noindent\makebox[\textwidth]{%
\begin{tabularx}{1.4\textwidth} { 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X | }
 \hline
 \textbf{Approach} & \textbf{Sensor Setup} & \textbf{Output} & \textbf{Time complexity} & \textbf{Key Limitations} \\
 \hline
 \textbf{OpenPose} 2021 \cite{openPose} (\ref{method_openpose}) & markerless \newline single RGB camera for 2D \break multi-view RGB cameras for 3D & multi person 2D keypoints including foot and hands. Option for single person 3D keypoints via triangulation in a multi-view setup & real-time & 3D capabilities very limited; Body occlusion and high crowded images lead to false keypoints. \\
 \hline
 \textbf{DeMoCap} 2021 \cite{chatzitofis2021democap} (\ref{method_democap}) & 53 body markers \newline  + 4 low-cost depth cameras  & single person 3D pose in SMPL & real-time & limited capture space due to 4m range limit of sensors \newline required 53 markers or retraining \\
 \hline
 \textbf{XNect} 2020 \cite{mehta_xnect_2020} (\ref{method_xnect}) & single RGB camera & multi person 3D pose as Skeleton & real-time & Occlusion of neck leads to complete loss of person; Issues with complex interactions; person mismatch between frames \\
 \hline
 \textbf{Ultra Inertial Poser} 2024 \cite{UltraInertialPoser} (\ref{method_uip}) &  6 wearable trackers with IMU + UWB & single person 3D pose in SMPL & real-time & clear visible jitter artifacts; low tracking position accuracy (magnitude of 5-11 cm)  \\
 \hline
 \textbf{MAMMA} 2025 \cite{cuevas2025mamma} (\ref{method_mamma}) & 33 high quality RGB cameras & multi person 3D pose in SMPL-X & offline; processing duration around 3x than recording & high number of camera required; low mobility \\
 \hline
 \textbf{SimPoE} 2021 \cite{simpoe} & single RGB camera & single person 3D pose (compatible with body mesh models like SMPL)& real-time & requires 3D scene modeling; no comparison to multi-view approaches; no interaction / complex scenes  \\
 \hline
 \textbf{Avatarpose} 2024 \cite{leonardis_avatarpose_2025} & sparse set of RGB cameras & multi person 3D pose in SMPL & offline & no hand tracking; issues when initial pose is in opposite direction  \\
 \hline
\end{tabularx}%
}


\noindent\makebox[\textwidth]{%
\begin{tabularx}{1.4\textwidth} { 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X | }
 \hline
 \textbf{Approach} & \textbf{Sensor Setup} & \textbf{Output} & \textbf{Time complexity} & \textbf{Key Limitations} \\
 \hline
 \textbf{PACE} 2024 \cite{PACE} & single moveable RGB camera in the wild & multi person 3D pose in SMPL and global splace & offline & assumption of planar ground; fails at similar scenarios where SLAM methods fail like similar backgrounds\\
 \hline
%  \textbf{EasyMocap} \cite{easymocap} \cite{shuai2022multinb} \cite{lin2022efficient} \cite{dong2021fast} \cite{dong2020motion} \cite{peng2021neural} \cite{fang2021mirrored} & single or multi view RGB camers & multi person 3D pose in SMPL & & \\
%  \hline
% \textbf{EgoPoser} 2024 \cite{jiang2024egoposer}  & egocentric wide angle RGB camera from HMD devices & single person 3D pose in SMPL & real-time & \\
%  \hline
%  \textbf{FLAG} 2022 \cite{FLAG} & & & & \\
%  \hline
%  \textbf{EgoPoser} \cite{jiang2024egoposer} & & & & worse than SimPoe\\
%  \hline
%  \textbf{TransPose} 2021 \cite{TransPose} & 6 wearable IMU sensors & & real-time & \\
%  \hline
%  \textbf{EgoLocate} \cite{EgoLocate2023} & & & & \\
%  \hline
%  \textbf{Sony Mocopi} \cite{mocopi} & & & & \\
%  \hline
%  \textbf{Physical Inertial Poser (PIP)} \cite{PIPCVPR2022} & & & & \\
% \hline
\end{tabularx}%
}

\vspace{0.5cm}
\textbf{Table 1:} Qualitative comparisons including general sensor setup and key limitations.
\label{tab:comparison}

\pagebreak
\thispagestyle{empty}
\begin{landscape}

\centering
\footnotesize
\renewcommand{\arraystretch}{1.6}
\begin{tabularx}{\linewidth} { 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X | }
 \hline
 \textbf{Dataset} & \textbf{DeMoCap} & \textbf{XNect} & \textbf{Ultra Inertial Poser} & \textbf{MAMMA} & \textbf{SimPoE} & \textbf{Avatarpose} & \textbf{PACE} \\
 \hline
 \textbf{Hi4D} &  &  &  & 13.43 &  & 32.1 &  \\
 \hline
 \textbf{RICH} &  &  &  & 23.6 &  &  & 46.2 \\
 \hline
 \textbf{Harmony4D} &  &  &  & 42.23 &  &  &  \\
 \hline
 \textbf{CHI3D} &  &  &  & 31.14 &  & 32.98 &  \\
 \hline
 \textbf{MOYO} &  &  &  & 22.6 &  &  &  \\
 \hline
 \textbf{Human3.6M} &  & 63.6 &  &  & 56.7 &  &  \\
 \hline
 \textbf{MPI-INF-3DHP} &  & 115.0 &  &  &  &  &  \\
 \hline
 \textbf{DanceDB (IMU)} &  &  & 74.5 &  &  &  &  \\
 \hline
 \textbf{DIP-IMU} &  &  & 50.2 &  &  &  &  \\
 \hline
 \textbf{SFU Dataset} & 45.28 &  &  &  &  &  &  \\
 \hline
\end{tabularx}

\vspace{0.5cm}
\textbf{Table 2:} Quantitative comparison for tracking accuracy. Values represent Mean Per-Joint Position Error (MPJPE) in millimeters. Note that the evaluation on each approach was not standardized due to the large difference of the approaches. So the quantitative numbers should only be taken as a relative comparison between the approaches. For detailed benchmark results refer to the individual papers.
\label{tab:dataset}


% MPJPE 
% DEMOCAP
% SFU Dataset: MPJPE: 45,28 ; mAP50 75,46%
% cite AMASS_SFU
% 

% XNECT:
% MuPoTS-3D: 3DPCK with 150mm: 70,4
% Human3.6m: MPJPE 63,6
% MPI-INF-3DHP: MJPE 115.0


% Ultra Intertial Poser
% Joint position error MPJPE: 
% DanceDB: 7.45 cm
% DIP-IMU: 5.02 cm
% 
% 

% MAMMA 
% Hi4D: 13.43 mm
% RICH: 23.6 mm
% Harmony4D: 42.23 mm
% CHI3D: 31.14mm
% MOYO: 22.6mm


% SIMPoe
% Human 3.6M: 56.7mm

% Avatarposer
% Hi4D: 32.1mm
% CHI3D: 32.98mm

% PACE:
% RICH: 46.2mm 
\end{landscape}

\pagebreak
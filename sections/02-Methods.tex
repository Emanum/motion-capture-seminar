\section{Methods}
Keywords:
 monocular video, kinematics, global coordinates, dynamic cameras,  infills missing poses,

\subsection{Old but good}
OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields \cite{openPose}


XNect: real-time multi-person 3D motion capture with a single RGB camera \cite{mehta_xnect_2020}

\subsection{Non Real Time Approaches}
PACE: Human and Camera Motion Estimation from in-the-wild Videos \cite{PACE}

\subsection{Not categorized yet}
SimPoE: Simulated Character Control for 3D Human Pose Estimation \cite{simpoe}

VIBE: Video Inference for Human Body Pose and Shape Estimation \cite{VIBE}

Avatarpose \cite{leonardis_avatarpose_2025}


\subsection{Datasets}
AMASS: Archive of Motion Capture as Surface Shapes \cite{AMASS:ICCV:2019}

CMU Panoptic Studio datase

Human3.6M, HumanEva, and MPI-INF-3DHP dataset

\pagebreak

\subsection{OpenPose}

OpenPose is a popular open-source framework for real-time 2D multi-person and 3D single-person pose estimation with over 30K stars on GitHub.
There are multiple publications related to OpenPose. Convolutional Pose Machines \cite{openPose_4_wei2016cpm} started with a sequential architecture of convolutional neural networks that produces 2D belief map.
Each network describes a stage, the output of one stage is used as input for the next stage. By using a fully differentiable model they can use backpropagation for training. One issue with their approach is handling multiple people in the same image.\\
This is addressed in their next publication with Part Affinity Fields \cite{openPose_3_cao2017realtime}. A bottom-up instead of top-down approach is used. This means that first body parts are detected and afterwards assigned to an unknown number of people in the image.
They extend their sequential architecture with a two-branch approach. One branch predicts part affinity fields which encode position and orientation of limbs in a 2D vector field, another the confidence maps of body parts at a certain location. By using techniques from graph theory, they can match these body parts to the vector field and also combine them into a full body pose. With their architecture and greedy matching algorithm, they can achieve real-time multi-person 2D pose estimation. \\
In \cite{openPose_2_simon2017hand}, they propose a method that produces 3D motion capture results handling complex Occlusion scenarios. They build up on their previous models to predict 2D estimation via multiple camera angles. The result is then triangulated to get 3D results. 
However, compared to other approaches they go a step further and use these results as their training data together with their 2D estimations for a model which enables markerless 3D motion capture outputs on hands from single view RGB images.
In \cite{openPose}, they extend their 2D multi-person pose estimation to also include foot keypoints and facial landmarks, compared to their previous wor they only use Part Affinity Fields. Furthermore, this work also does not include any 3D motion capture cababilities. In their GitHub \cite{openPose_github} they provide an option to detect 3D keypoints including face, hand and foot features using multiple cameras. However, as it uses simple triangulation it has the limitation of a fixed camera setup and only works with a single person in the scene.

\subsection{DeMoCap}
Chatzitofis et al. proposed their 3D marker based motion capture system DeMoCap \cite{chatzitofis2021democap}. Their work includes the method itself, focused on providing a low-cost alternative to the classical optical marker based solutions by using consumer-grade infrared-depth camera only. In addition, they release the dataset used for training, which contains colored infrared and depth images with 3D pose and marker annotations in multiple views. The ground truth data comes from professional motion capture system with 24 cameras while the depth data comes from 4 stereo based depth sensors.\\
Their proposed method works as follows: First for each of the multi-view depth images 3D positions of the markers are extracted and normalized. This work as each marker reflects the infrared rays with a different intensity therefore identifying the marker. 
These markers together with joint heatmaps are fed into a fully convolutional network that transforms the markers to poses. This is done via their own 3D regression model. \\
While their methods performs quiet good even under complex scenarios on public datasets such as the SFU Dataset \cite{sfudataset} it comes with some serious limitations. 
Their used depth sensors have a distance limitation of 4 meters, which limits the capture space significantly especially compared to optical sensor based systems. Furthermore, the quantity of fast poses are challenging due to the low 30hz frequency of the sensor. 
With only 2-4 cameras required the system is fairly mobile and quickly to set up. However, it requires the actor to wear a special suit with 53 placed markers which make it far from ideal for applications in the wild.

\subsection{XNect}

Xnect \cite{mehta_xnect_2020} is a real-time markerless approach that is powered by a single monocular RGB based camera and can provide temporally coherent tracing tracking for multiple people in diverse scenes in the wild.

Their bottom up architecture start by applying a convolutional neural network which is trained to detect only fully visible features such as the joint itself, or it's parent/child. The networks output are 2D body joint heatmaps, part affinity fields to assign joints to individual persons as well as their own 3D pose encoder which uses local pixel information close to the 2D joint position. 
In a second step a lightweight fully connected neural network transform the 2D features and 3D encoder to 3D poses for each person. In addition, it uses previous learned 3D poses to handle occlusion within the scene. 
Lastly, a space-time skeletal model is applied to further improve the 3D pose. 
The result or the last two stages is used in combination with the results of previous frames to fit the 3D pose to a kinematic skeleton and therefore enforce temporal coherence.
The end result of the pipeline is a full skeletal pose with joint angles for each person.

While their CNN applies a novel CNN architecture called SelecSLS to achieve real-time performance, they mention that any CNN architecture for keypoint prediction can be used.
This makes their approach potentially be extended in the future by upcoming CNN network architectures. 

They compare their approach to Microsoft Kinect V2 which uses depth sensing cameras with similar and sometimes better qualitative performance especially for scenes including occlusions. 
In addition, results on common datasets such as MuPoTS-3D benchmark dataset (TODO cite) and the Panoptic dataset (TODO cite) are presented and compared to SingleShot [Mehta et al. 2018b], LCRNet [Rogez et al. 2017], MP3D [Dabral et al. 2019], LCRNet++ [Rogez et al. 2019] and PoseNet (https://arxiv.org/abs/1505.07427). The only approach that had a better score was PoseNet, however it comes with other limitations one of which is the lack of real-time performance. Their own CNN SelecSLS is compared to ResNet (TODO cite) with similar results while still also lacking real-time cababilities.
However, compared to multi-view capture systems XNect has far less accuracy. 
While occulusion is partly handled via their 2nd stage, it is far from perfect. 
Occlusion of the neck of a person result in the entire person not being detected regarding if large parts of the body is detected. Close human interactions like hugging also leads to inaccurate results. 
Using previous frames to match a skeleton can lean to issues for the first frames of occlusions. The simple person tracker also lead to mismatches between the capture and therefore a swap.


\subsection{MAMMA}


\section{Methods}
\label{chapter_methods}
Keywords:
 monocular video, kinematics, global coordinates, dynamic cameras,  infills missing poses,

\subsection{Non Real Time Approaches}
PACE: Human and Camera Motion Estimation from in-the-wild Videos \cite{PACE}

\subsection{Not categorized yet}
SimPoE: Simulated Character Control for 3D Human Pose Estimation \cite{simpoe}

VIBE: Video Inference for Human Body Pose and Shape Estimation \cite{VIBE}

Avatarpose \cite{leonardis_avatarpose_2025}


\pagebreak

\subsection{OpenPose - marker less 2D}

OpenPose is a popular open-source framework for real-time 2D multi-person and 3D single-person pose estimation with over 30K stars on GitHub.
There are multiple publications related to OpenPose. Convolutional Pose Machines \cite{openPose_4_wei2016cpm} started with a sequential architecture of convolutional neural networks that produces 2D belief map.
Each network describes a stage, the output of one stage is used as input for the next stage. By using a fully differentiable model they can use backpropagation for training. One issue with their approach is handling multiple people in the same image.\\
This is addressed in their next publication with Part Affinity Fields \cite{openPose_3_cao2017realtime}. A bottom-up instead of top-down approach is used. This means that first body parts are detected and afterwards assigned to an unknown number of people in the image.
They extend their sequential architecture with a two-branch approach. One branch predicts part affinity fields which encode position and orientation of limbs in a 2D vector field, another the confidence maps of body parts at a certain location. By using techniques from graph theory, they can match these body parts to the vector field and also combine them into a full body pose. With their architecture and greedy matching algorithm, they can achieve real-time multi-person 2D pose estimation. \\
In \cite{openPose_2_simon2017hand}, they propose a method that produces 3D motion capture results handling complex Occlusion scenarios. They build up on their previous models to predict 2D estimation via multiple camera angles. The result is then triangulated to get 3D results. 
However, compared to other approaches they go a step further and use these results as their training data together with their 2D estimations for a model which enables markerless 3D motion capture outputs on hands from single view RGB images.
In \cite{openPose}, they extend their 2D multi-person pose estimation to also include foot keypoints and facial landmarks, compared to their previous wor they only use Part Affinity Fields. Furthermore, this work also does not include any 3D motion capture cababilities. In their GitHub \cite{openPose_github} they provide an option to detect 3D keypoints including face, hand and foot features using multiple cameras. However, as it uses simple triangulation it has the limitation of a fixed camera setup and only works with a single person in the scene.

\subsection{DeMoCap - marker based 3D} 
Chatzitofis et al. proposed their 3D marker based motion capture system DeMoCap \cite{chatzitofis2021democap}. Their work includes the method itself, focused on providing a low-cost alternative to the classical optical marker based solutions by using consumer-grade infrared-depth camera only. In addition, they release the dataset used for training, which contains colored infrared and depth images with 3D pose and marker annotations in multiple views. The ground truth data comes from professional motion capture system with 24 cameras while the depth data comes from 4 stereo based depth sensors.\\
Their proposed method works as follows: First for each of the multi-view depth images 3D positions of the markers are extracted and normalized. This work as each marker reflects the infrared rays with a different intensity therefore identifying the marker. 
These markers together with joint heatmaps are fed into a fully convolutional network that transforms the markers to poses. This is done via their own 3D regression model. \\
While their methods performs quiet good even under complex scenarios on public datasets such as the SFU Dataset \cite{sfudataset} it comes with some serious limitations. 
Their used depth sensors have a distance limitation of 4 meters, which limits the capture space significantly especially compared to optical sensor based systems. Furthermore, the quantity of fast poses are challenging due to the low 30hz frequency of the sensor. 
With only 2-4 cameras required the system is fairly mobile and quickly to set up. However, it requires the actor to wear a special suit with 53 placed markers which make it far from ideal for applications in the wild.

\subsection{XNect - marker less 3D}

Xnect \cite{mehta_xnect_2020} is a real-time marker less approach that is powered by a single monocular RGB based camera and can provide temporally coherent tracing tracking for multiple people in diverse scenes in the wild.

Their bottom up architecture start by applying a convolutional neural network which is trained to detect only fully visible features such as the joint itself, or it's parent/child. The networks output are 2D body joint heatmaps, part affinity fields to assign joints to individual persons as well as their own 3D pose encoder which uses local pixel information close to the 2D joint position. 
In a second step a lightweight fully connected neural network transform the 2D features and 3D encoder to 3D poses for each person. In addition, it uses previous learned 3D poses to handle occlusion within the scene. 
Lastly, a space-time skeletal model is applied to further improve the 3D pose. 
The result or the last two stages is used in combination with the results of previous frames to fit the 3D pose to a kinematic skeleton and therefore enforce temporal coherence.
The end result of the pipeline is a full skeletal pose with joint angles for each person.

While their CNN applies a novel CNN architecture called SelecSLS to achieve real-time performance, they mention that any CNN architecture for keypoint prediction can be used.
This makes their approach potentially be extended in the future by upcoming CNN network architectures. 

They compare their approach to Microsoft Kinect V2 which uses depth sensing cameras with similar and sometimes better qualitative performance especially for scenes including occlusions. 
In addition, results on common datasets such as MuPoTS-3D benchmark dataset \cite{singleshotmultiperson2018} and the Panoptic dataset \cite{Joo_2017_TPAMI} \cite{Joo_2015_ICCV} are presented and compared to SingleShot \cite{singleshotmultiperson2018}, LCRNet \cite{shi2024tro}, MP3D \cite{mp3d}, LCRNet++ \cite{lcrnetplusplus} and PoseNet \cite{resnet}. The only approach that had a better score was PoseNet, however it comes with other limitations one of which is the lack of real-time performance. Their own CNN SelecSLS is compared to ResNet \cite{resnet} with similar results while still also lacking real-time cababilities.
However, compared to multi-view capture systems XNect has far less accuracy. 
While occulusion is partly handled via their 2nd stage, it is far from perfect. 
Occlusion of the neck of a person result in the entire person not being detected regarding if large parts of the body is detected. Close human interactions like hugging also leads to inaccurate results. 
Using previous frames to match a skeleton can lean to issues for the first frames of occlusions. The simple person tracker also lead to mismatches between the capture and therefore a swap.
% https://universaar.uni-saarland.de/bitstream/20.500.11880/29908/1/thesis.pdf

\subsection{Ultra Inertial Poser - IMU based 3D} 

Tracking systems using IMU sensor are not new. We have seen various approaches like TransPose \cite{TransPose} and EgoLocate \cite{EgoLocate2023} as well as commercial products like Sony Mocopi \cite{mocopi}. One issue with IMU sensors is that they are prone to drift and jitter and therefore are not as reliable as other tracking methods. 
Ultra Inertial Poser \cite{UltraInertialPoser} attempt to increase tracking accuracy and reliability by combining the IMU data with ultra-wideband (UWB) ranging sensor data. 
Their systems work with six embedded trackers each combing with a 6-DoF IMU and a UWB sensor plus emitter to measure inter-sensor distances.
UWB signals use a large bandwidth together with short waveforms, based on that physical property there exist multiple protocols used in signal processing to measure distance: Two-Way Ranging, Time Difference of Arrival and Angle of Arrival.
One challenge with UWB is to handle noise, which arises when objects, such as body joints, prevent a direct line of sight between sensors. Ultra Inertial Poser uses an Extended Kalman Filter to reduce the noise.

Each tracker is equipped with an integrated microcontroller which samples raw IMU data and an estimated distance to other trackers. The raw data is streamed via Bluetooth low energy to a host computer which performs the rest of their tracking pipeline.
First a LSTM network together with a distance attention graph convolutional network (DA-GCN) is used to determine the positions of the trackers in 3D space.
Afterwards a kinematics decoder based on the work Physical Inertial Poser (PIP) \cite{PIPCVPR2022} is used to estimate the final SMPL pose parameters as well as a global translation relative to an initial T-Pose calibration performed by the user.

Their evaluation shows that their approach with the combination of IMU and UWB data for tracking performs better than other IMU only based methods like PIP \cite{PIPCVPR2022}.
They also confirmed that the IMU sensor quality highly influences drift errors. As other methods performed way worse on their dataset gathered with inexpensive IMU than synthetic datasets or data gathered from high-end sensors.
The difference was bigger for small acceleration motions as pure IMU methods tend to perfom worse here. Another advantage of the inclusion of UWB distance data is that pure IMU methods tend to mistake sitting poses with standing poses where a combined data approach can correctly classify those cases. Furthermore, their LSTM and DA-GCN network were able to reduce jitter error.

However, they do not compare their approach against camera based tracking methods, which are way less prone to jitter in general. A video presentation \cite{UltraInertialPoser_YT} of their work shows a clear improvement compared to pure IMU based methods. Nevertheless, compared to the ground truth there is a still a large jitter movement visible to the naked eye.


% Issue: UWB sensitive to temperature
% Issues: missing Real world, drift, hard poses not captured as no training data


\subsection{MAMMA}

Traditional "gold standard" motion capture studio use marker based methods to produce sub-millimeter accurate tracking. MAMMA (Markerless and Accurate Multi-person Motion Automatic capture) \cite{cuevas2025mamma} claim to achieve similar results with a markerless approach in a studio setup including 32 RGB cameras.
Their algorithm only uses multiple multi view videos as input and estimate a dense 2D surface for multiple people in complex scenarios. The motion capture data is predicted in the SMPL-X format \cite{SMPLX} and includes hand gestures. \\
A network consisting of vision transformer (ViT) and a CNN is trained to extract images features. They use a technique called landmark queries, which uses multiple embeddings, to further increase generalization of the network. This increases accuracy for challenging poses that are not present in the training data. Multiple embeddings are possible because their technique can use the entire video input compared to just relying on marker positions.
The detected features of the ViT and CNN are combined and used first in a Transformer Decoder and secondly in an MLP network to predict 2D landmarks for each camera view. These landmarks include position via pixel coordinates as well as a numerical value stating the probability of the feature being visible on camera. 
In The final stage these landmarks are fit to a SMPL-X 3D body including multiple stages of optimization.\\
They train their network on a pure synthetic dataset which simulating their 32 camera studio setup, consisting of single person scenes, complex interaction scenes including dance scenes with two persons and a subset focusing on hand articulations. \\
An evaluation on the Hi4D dataset \cite{yin2023hi4d} show that their method outperform other approaches. The best in class, Avatarpose \cite{leonardis_avatarpose_2025} was beaten in tracking accuracy with a value of 13.43 to 32.10 Mean Per-Joint Position Error (mm).
Furthermore, a direct comparison to Vicon \cite{VICON} is performed using a real evaluation dataset. Results show that there is only a minimal 1.611mm difference compared to the "gold standard". This make their technique accurate enough for many applications. An interesting comparison is also done in terms of usability and cost. Capturing a dataset with Vicon requires manual post-processing. 3 skilled technicians worked in total close to 47 hours to perform the cleanup of a 24-minute scene. Computation for the final SMPL-X format took another 25 hours. MAMMA does not require manual post-processing and took about 8 hours on an RTX-4090 to compute the final motion tracking result.\\
As for limitations heavenly occluded areas are still a problem and can cause flickering or over-smoothing. In addition, the accuracy on hand tracking is not ideal. Compared to other methods it is also not useable in real-time scenarios and a fixed studio setup with a larger amount of camera is required. 